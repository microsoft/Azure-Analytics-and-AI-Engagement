{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Why Segmentation\r\n",
        "\r\n",
        "If you want to increase retention rate, you can do a segmentation based on churn probability and take actions. But there are very common and useful segmentation methods as well. Now we are going to implement one of them to our business: RFM.\r\n",
        "\r\n",
        "#### RFM stands for Recency - Frequency - Monetary Value. Theoretically we will have segments like below:\r\n",
        "\r\n",
        "- **Low Value:** Customers who are less active than others, not very frequent buyer/visitor and generates very low - zero - maybe negative revenue.\r\n",
        "\r\n",
        "- **Mid Value:** In the middle of everything. Often using our platform (but not as much as our High Values), fairly frequent and generates moderate revenue.\r\n",
        "\r\n",
        "- **High Value:** The group we don’t want to lose. High Revenue, Frequency and low Inactivity."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from __future__ import division\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1643309271169
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge cufflinks-py\n",
        "!conda install -c plotly chart-studio\n",
        "\n",
        "#import chart_studio\n",
        "import plotly\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyoff\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\nSolving environment: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n\n## Package Plan ##\n\n  environment location: /anaconda\n\n  added / updated specs:\n    - cufflinks-py\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    attrs-21.4.0               |     pyhd8ed1ab_0          49 KB  conda-forge\n    backcall-0.2.0             |     pyh9f0ad1d_0          13 KB  conda-forge\n    backports-1.0              |             py_2           4 KB  conda-forge\n    backports.functools_lru_cache-1.6.4|     pyhd8ed1ab_0           9 KB  conda-forge\n    bleach-4.1.0               |     pyhd8ed1ab_0         121 KB  conda-forge\n    ca-certificates-2021.10.8  |       ha878542_0         139 KB  conda-forge\n    certifi-2021.10.8          |   py39hf3d152e_1         145 KB  conda-forge\n    chart-studio-1.1.0         |     pyh9f0ad1d_0          51 KB  conda-forge\n    colorlover-0.3.0           |             py_0          12 KB  conda-forge\n    conda-4.11.0               |   py39hf3d152e_0        16.8 MB  conda-forge\n    cufflinks-py-0.17.3        |             py_0           4 KB  conda-forge\n    decorator-5.1.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n    defusedxml-0.7.1           |     pyhd8ed1ab_0          23 KB  conda-forge\n    entrypoints-0.3            |  pyhd8ed1ab_1003           8 KB  conda-forge\n    importlib-metadata-4.10.1  |   py39hf3d152e_0          33 KB  conda-forge\n    importlib_resources-5.4.0  |     pyhd8ed1ab_0          21 KB  conda-forge\n    ipykernel-5.5.5            |   py39hef51801_0         167 KB  conda-forge\n    ipython-7.31.1             |   py39hf3d152e_0         1.1 MB  conda-forge\n    ipython_genutils-0.2.0     |             py_1          21 KB  conda-forge\n    ipywidgets-7.6.5           |     pyhd8ed1ab_0         101 KB  conda-forge\n    jedi-0.18.1                |   py39hf3d152e_0         992 KB  conda-forge\n    jinja2-3.0.3               |     pyhd8ed1ab_0          99 KB  conda-forge\n    jsonschema-4.4.0           |     pyhd8ed1ab_0          57 KB  conda-forge\n    jupyter_client-7.1.2       |     pyhd8ed1ab_0          90 KB  conda-forge\n    jupyter_core-4.9.1         |   py39hf3d152e_1          80 KB  conda-forge\n    jupyterlab_widgets-1.0.2   |     pyhd8ed1ab_0         130 KB  conda-forge\n    libblas-3.9.0              |11_linux64_openblas          12 KB  conda-forge\n    libcblas-3.9.0             |11_linux64_openblas          11 KB  conda-forge\n    libgfortran-ng-11.2.0      |      h69a702a_12          19 KB  conda-forge\n    libgfortran5-11.2.0        |      h5c6108e_12         1.7 MB  conda-forge\n    liblapack-3.9.0            |11_linux64_openblas          11 KB  conda-forge\n    libopenblas-0.3.17         |pthreads_h8fe5266_1         9.2 MB  conda-forge\n    libsodium-1.0.18           |       h36c2ea0_1         366 KB  conda-forge\n    markupsafe-2.0.1           |   py39h3811e60_0          22 KB  conda-forge\n    matplotlib-inline-0.1.3    |     pyhd8ed1ab_0          11 KB  conda-forge\n    mistune-0.8.4              |py39h3811e60_1004          54 KB  conda-forge\n    nbconvert-5.6.1            |     pyhd8ed1ab_2         373 KB  conda-forge\n    nbformat-5.1.3             |     pyhd8ed1ab_0          47 KB  conda-forge\n    nest-asyncio-1.5.4         |     pyhd8ed1ab_0           9 KB  conda-forge\n    notebook-5.7.11            |   py39hf3d152e_0         7.9 MB  conda-forge\n    numpy-1.20.3               |   py39hdbf815f_1         5.8 MB  conda-forge\n    openssl-1.1.1m             |       h7f8727e_0         2.5 MB\n    packaging-21.3             |     pyhd8ed1ab_0          36 KB  conda-forge\n    pandas-1.2.3               |   py39hde0f152_0        12.1 MB  conda-forge\n    pandocfilters-1.5.0        |     pyhd8ed1ab_0          11 KB  conda-forge\n    parso-0.8.3                |     pyhd8ed1ab_0          69 KB  conda-forge\n    pexpect-4.8.0              |     pyh9f0ad1d_2          47 KB  conda-forge\n    pickleshare-0.7.5          |          py_1003           9 KB  conda-forge\n    plotly-5.5.0               |     pyhd8ed1ab_0         6.9 MB  conda-forge\n    prometheus_client-0.13.0   |     pyhd8ed1ab_0          47 KB  conda-forge\n    prompt-toolkit-3.0.25      |     pyha770c72_0         249 KB  conda-forge\n    ptyprocess-0.7.0           |     pyhd3deb0d_0          16 KB  conda-forge\n    pygments-2.11.2            |     pyhd8ed1ab_0         796 KB  conda-forge\n    pyparsing-3.0.7            |     pyhd8ed1ab_0          79 KB  conda-forge\n    pyrsistent-0.18.0          |   py39heee7806_0          94 KB\n    python-cufflinks-0.17.3    |             py_0          59 KB  conda-forge\n    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n    pytz-2021.3                |     pyhd8ed1ab_0         242 KB  conda-forge\n    pyzmq-19.0.2               |   py39hb69f2a1_2         479 KB  conda-forge\n    retrying-1.3.3             |             py_2          11 KB  conda-forge\n    send2trash-1.8.0           |     pyhd8ed1ab_0          17 KB  conda-forge\n    tenacity-8.0.1             |     pyhd8ed1ab_0          21 KB  conda-forge\n    terminado-0.13.1           |   py39hf3d152e_0          27 KB  conda-forge\n    testpath-0.5.0             |     pyhd8ed1ab_0          86 KB  conda-forge\n    tornado-6.1                |   py39h3811e60_1         646 KB  conda-forge\n    traitlets-5.1.1            |     pyhd8ed1ab_0          82 KB  conda-forge\n    wcwidth-0.2.5              |     pyh9f0ad1d_2          33 KB  conda-forge\n    webencodings-0.5.1         |             py_1          12 KB  conda-forge\n    widgetsnbextension-3.5.2   |   py39hf3d152e_1         1.3 MB  conda-forge\n    zeromq-4.3.4               |       h9c3ff4c_0         352 KB  conda-forge\n    zipp-3.7.0                 |     pyhd8ed1ab_0          12 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        72.2 MB\n\nThe following NEW packages will be INSTALLED:\n\n  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0\n  backcall           conda-forge/noarch::backcall-0.2.0-pyh9f0ad1d_0\n  backports          conda-forge/noarch::backports-1.0-py_2\n  backports.functoo~ conda-forge/noarch::backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0\n  bleach             conda-forge/noarch::bleach-4.1.0-pyhd8ed1ab_0\n  chart-studio       conda-forge/noarch::chart-studio-1.1.0-pyh9f0ad1d_0\n  colorlover         conda-forge/noarch::colorlover-0.3.0-py_0\n  cufflinks-py       conda-forge/noarch::cufflinks-py-0.17.3-py_0\n  decorator          conda-forge/noarch::decorator-5.1.1-pyhd8ed1ab_0\n  defusedxml         conda-forge/noarch::defusedxml-0.7.1-pyhd8ed1ab_0\n  entrypoints        conda-forge/noarch::entrypoints-0.3-pyhd8ed1ab_1003\n  importlib-metadata conda-forge/linux-64::importlib-metadata-4.10.1-py39hf3d152e_0\n  importlib_resourc~ conda-forge/noarch::importlib_resources-5.4.0-pyhd8ed1ab_0\n  ipykernel          conda-forge/linux-64::ipykernel-5.5.5-py39hef51801_0\n  ipython            conda-forge/linux-64::ipython-7.31.1-py39hf3d152e_0\n  ipython_genutils   conda-forge/noarch::ipython_genutils-0.2.0-py_1\n  ipywidgets         conda-forge/noarch::ipywidgets-7.6.5-pyhd8ed1ab_0\n  jedi               conda-forge/linux-64::jedi-0.18.1-py39hf3d152e_0\n  jinja2             conda-forge/noarch::jinja2-3.0.3-pyhd8ed1ab_0\n  jsonschema         conda-forge/noarch::jsonschema-4.4.0-pyhd8ed1ab_0\n  jupyter_client     conda-forge/noarch::jupyter_client-7.1.2-pyhd8ed1ab_0\n  jupyter_core       conda-forge/linux-64::jupyter_core-4.9.1-py39hf3d152e_1\n  jupyterlab_widgets conda-forge/noarch::jupyterlab_widgets-1.0.2-pyhd8ed1ab_0\n  libblas            conda-forge/linux-64::libblas-3.9.0-11_linux64_openblas\n  libcblas           conda-forge/linux-64::libcblas-3.9.0-11_linux64_openblas\n  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-11.2.0-h69a702a_12\n  libgfortran5       conda-forge/linux-64::libgfortran5-11.2.0-h5c6108e_12\n  liblapack          conda-forge/linux-64::liblapack-3.9.0-11_linux64_openblas\n  libopenblas        conda-forge/linux-64::libopenblas-0.3.17-pthreads_h8fe5266_1\n  libsodium          conda-forge/linux-64::libsodium-1.0.18-h36c2ea0_1\n  markupsafe         conda-forge/linux-64::markupsafe-2.0.1-py39h3811e60_0\n  matplotlib-inline  conda-forge/noarch::matplotlib-inline-0.1.3-pyhd8ed1ab_0\n  mistune            conda-forge/linux-64::mistune-0.8.4-py39h3811e60_1004\n  nbconvert          conda-forge/noarch::nbconvert-5.6.1-pyhd8ed1ab_2\n  nbformat           conda-forge/noarch::nbformat-5.1.3-pyhd8ed1ab_0\n  nest-asyncio       conda-forge/noarch::nest-asyncio-1.5.4-pyhd8ed1ab_0\n  notebook           conda-forge/linux-64::notebook-5.7.11-py39hf3d152e_0\n  numpy              conda-forge/linux-64::numpy-1.20.3-py39hdbf815f_1\n  packaging          conda-forge/noarch::packaging-21.3-pyhd8ed1ab_0\n  pandas             conda-forge/linux-64::pandas-1.2.3-py39hde0f152_0\n  pandocfilters      conda-forge/noarch::pandocfilters-1.5.0-pyhd8ed1ab_0\n  parso              conda-forge/noarch::parso-0.8.3-pyhd8ed1ab_0\n  pexpect            conda-forge/noarch::pexpect-4.8.0-pyh9f0ad1d_2\n  pickleshare        conda-forge/noarch::pickleshare-0.7.5-py_1003\n  plotly             conda-forge/noarch::plotly-5.5.0-pyhd8ed1ab_0\n  prometheus_client  conda-forge/noarch::prometheus_client-0.13.0-pyhd8ed1ab_0\n  prompt-toolkit     conda-forge/noarch::prompt-toolkit-3.0.25-pyha770c72_0\n  ptyprocess         conda-forge/noarch::ptyprocess-0.7.0-pyhd3deb0d_0\n  pygments           conda-forge/noarch::pygments-2.11.2-pyhd8ed1ab_0\n  pyparsing          conda-forge/noarch::pyparsing-3.0.7-pyhd8ed1ab_0\n  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py39heee7806_0\n  python-cufflinks   conda-forge/noarch::python-cufflinks-0.17.3-py_0\n  python-dateutil    conda-forge/noarch::python-dateutil-2.8.2-pyhd8ed1ab_0\n  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39\n  pytz               conda-forge/noarch::pytz-2021.3-pyhd8ed1ab_0\n  pyzmq              conda-forge/linux-64::pyzmq-19.0.2-py39hb69f2a1_2\n  retrying           conda-forge/noarch::retrying-1.3.3-py_2\n  send2trash         conda-forge/noarch::send2trash-1.8.0-pyhd8ed1ab_0\n  tenacity           conda-forge/noarch::tenacity-8.0.1-pyhd8ed1ab_0\n  terminado          conda-forge/linux-64::terminado-0.13.1-py39hf3d152e_0\n  testpath           conda-forge/noarch::testpath-0.5.0-pyhd8ed1ab_0\n  tornado            conda-forge/linux-64::tornado-6.1-py39h3811e60_1\n  traitlets          conda-forge/noarch::traitlets-5.1.1-pyhd8ed1ab_0\n  wcwidth            conda-forge/noarch::wcwidth-0.2.5-pyh9f0ad1d_2\n  webencodings       conda-forge/noarch::webencodings-0.5.1-py_1\n  widgetsnbextension conda-forge/linux-64::widgetsnbextension-3.5.2-py39hf3d152e_1\n  zeromq             conda-forge/linux-64::zeromq-4.3.4-h9c3ff4c_0\n  zipp               conda-forge/noarch::zipp-3.7.0-pyhd8ed1ab_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates    anaconda::ca-certificates-2020.10.14-0 --> conda-forge::ca-certificates-2021.10.8-ha878542_0\n  certifi            pkgs/main::certifi-2021.10.8-py39h06a~ --> conda-forge::certifi-2021.10.8-py39hf3d152e_1\n  openssl                                 1.1.1l-h7f8727e_0 --> 1.1.1m-h7f8727e_0\n\nThe following packages will be SUPERSEDED by a higher-priority channel:\n\n  conda              pkgs/main::conda-4.11.0-py39h06a4308_0 --> conda-forge::conda-4.11.0-py39hf3d152e_0\n\n\nProceed ([y]/n)? "
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1631287508343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inititate Plotly\r\n",
        "pyoff.init_notebook_mode()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287520865
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### As the methodology, we need to calculate Recency, Frequency and Monetary Value (we will call it Revenue from now on) and apply unsupervised machine learning to identify different groups (clusters) for each. Let’s jump into coding and see how to do RFM Clustering."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\r\n",
        "#data source: https://www.kaggle.com/vijayuv/onlineretail\r\n",
        "tx_data = pd.read_csv('data.csv', encoding= 'unicode_escape')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287521420
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets see the dataset\r\n",
        "tx_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287522317
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the string date field to datetime\r\n",
        "tx_data['InvoiceDate'] = pd.to_datetime(tx_data['InvoiceDate'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287522891
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_data['InvoiceDate'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287523459
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we will be using only UK data (we can filter data for others countries also)\r\n",
        "tx_uk = tx_data.query(\"Country=='United Kingdom'\").reset_index(drop=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287524017
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a generic user dataframe to keep CustomerID and new segmentation scores\n",
        "tx_user = pd.DataFrame(tx_data['CustomerID'].unique())\n",
        "tx_user.columns = ['CustomerID']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287524385
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recency\r\n",
        "\r\n",
        "To calculate recency, we need to find out most recent purchase date of each customer and see how many days they are inactive for. After having no. of inactive days for each customer, we will apply K-means* clustering to assign customers a recency score."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#get the max purchase date for each customer and create a dataframe with it\r\n",
        "tx_max_purchase = tx_uk.groupby('CustomerID').InvoiceDate.max().reset_index()\r\n",
        "tx_max_purchase.columns = ['CustomerID','MaxPurchaseDate']\r\n",
        "tx_max_purchase"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287524928
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we take our observation point as the max invoice date in our dataset\r\n",
        "tx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287525451
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge this dataframe to our new user dataframe\r\n",
        "#Our new dataframe tx_user contains recency data now:\r\n",
        "\r\n",
        "tx_user = pd.merge(tx_user, tx_max_purchase[['CustomerID','Recency']], on='CustomerID')\r\n",
        "tx_user.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287525982
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To get a snapshot about how recency looks like, we can use pandas’ .describe() method. \r\n",
        "#It shows mean, min, max, count and percentiles of our data.\r\n",
        "tx_user.Recency.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287526528
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that even though the average is 90 day recency, median is 49."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### to show us how is the distribution of recency across our customers."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot a recency histogram\n",
        "plot_data = [\n",
        "    go.Histogram(\n",
        "        x=tx_user['Recency']\n",
        "    )\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        title='Recency'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287527194
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now it is the fun part. We are going to apply K-means clustering to assign a recency score. But we should tell how many clusters we need to K-means algorithm. \r\n",
        "\r\n",
        "- To find it out, we will apply Elbow Method. Elbow Method simply tells the optimal cluster number for optimal inertia. Code snippet and Inertia graph are as follows:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "sse={}\n",
        "tx_recency = tx_user[['Recency']]\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_recency)\n",
        "    tx_recency[\"clusters\"] = kmeans.labels_\n",
        "    sse[k] = kmeans.inertia_ \n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287527783
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it looks like 3 is the optimal one. we can go ahead with less or more clusters. We will be selecting 4 for this example:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#build 4 clusters for recency and add it to dataframe\n",
        "\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Recency']])\n",
        "tx_user['RecencyCluster'] = kmeans.predict(tx_user[['Recency']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287528583
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have calculated clusters and assigned them to each Customer in our dataframe tx_user."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.groupby('RecencyCluster')['Recency'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287529213
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for ordering cluster numbers\n",
        "\n",
        "def order_cluster(cluster_field_name, target_field_name,df,ascending):\n",
        "    new_cluster_field_name = 'new_' + cluster_field_name\n",
        "    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n",
        "    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n",
        "    df_new['index'] = df_new.index\n",
        "    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n",
        "    df_final = df_final.drop([cluster_field_name],axis=1)\n",
        "    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n",
        "    return df_final\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287529754
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have added one function to our code which is order_cluster(). K-means assigns clusters as numbers but not in an ordered way. We can’t say cluster 0 is the worst and cluster 4 is the best. order_cluster() method does this for us and our new dataframe looks much neater:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)\r\n",
        "tx_user.groupby('RecencyCluster')['Recency'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287530361
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Great! 3 covers most recent customers whereas 0 has the most inactive ones.\r\n",
        "#### Let’s apply same for Frequency and Revenue."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency\r\n",
        "\r\n",
        "To create frequency clusters, we need to find total number orders for each customer. First calculate this and see how frequency look like in our customer database:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#get order counts for each user and create a dataframe with it\r\n",
        "tx_frequency = tx_uk.groupby('CustomerID').InvoiceDate.count().reset_index()\r\n",
        "tx_frequency.columns = ['CustomerID','Frequency']\r\n",
        "tx_frequency.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287531111
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add this data to our main dataframe\r\n",
        "\r\n",
        "tx_user = pd.merge(tx_user, tx_frequency, on='CustomerID')\r\n",
        "tx_user.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287531660
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.Frequency.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287532248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the histogram\n",
        "\n",
        "plot_data = [\n",
        "    go.Histogram(\n",
        "        x=tx_user.query('Frequency < 1000')['Frequency']\n",
        "    )\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        title='Frequency'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287532787
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sse={}\n",
        "tx_frequency = tx_user[['Frequency']]\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_frequency)\n",
        "    tx_frequency[\"clusters\"] = kmeans.labels_\n",
        "    sse[k] = kmeans.inertia_ \n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287533397
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the same logic for having frequency clusters and assign this to each customer:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Frequency']])\n",
        "tx_user['FrequencyCluster'] = kmeans.predict(tx_user[['Frequency']])\n",
        "tx_user.groupby('FrequencyCluster')['Frequency'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287533971
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Characteristics of our frequency clusters look like below:\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user = order_cluster('FrequencyCluster', 'Frequency',tx_user,True)\r\n",
        "tx_user"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287534531
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### As the same notation as recency clusters, high frequency number indicates better customers."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Revenue\r\n",
        "\r\n",
        "Let’s see how our customer database looks like when we cluster them based on revenue. We will calculate revenue for each customer, plot a histogram and apply the same clustering method."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate revenue for each customer\r\n",
        "tx_uk['Revenue'] = tx_uk['UnitPrice'] * tx_uk['Quantity']\r\n",
        "tx_revenue = tx_uk.groupby('CustomerID').Revenue.sum().reset_index()\r\n",
        "tx_revenue.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287535066
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#merge it with our main dataframe\r\n",
        "tx_user = pd.merge(tx_user, tx_revenue, on='CustomerID')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287535579
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.Revenue.describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287536142
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the histogram\n",
        "\n",
        "plot_data = [\n",
        "    go.Histogram(\n",
        "        x=tx_user.query('Revenue < 10000')['Revenue']\n",
        "    )\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        title='Monetary Value'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287536699
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have some customers with negative revenue as well. Let’s continue and apply k-means clustering:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287537213
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sse={}\n",
        "tx_revenue = tx_user[['Revenue']]\n",
        "for k in range(1, 10):\n",
        "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(tx_revenue)\n",
        "    tx_revenue[\"clusters\"] = kmeans.labels_\n",
        "    sse[k] = kmeans.inertia_ \n",
        "plt.figure()\n",
        "plt.plot(list(sse.keys()), list(sse.values()))\n",
        "plt.xlabel(\"Number of cluster\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287537804
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#apply clustering\n",
        "\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "kmeans.fit(tx_user[['Revenue']])\n",
        "tx_user['RevenueCluster'] = kmeans.predict(tx_user[['Revenue']])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287538341
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#order the cluster numbers\r\n",
        "\r\n",
        "tx_user = order_cluster('RevenueCluster', 'Revenue',tx_user,True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287538866
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.groupby('RevenueCluster')['Revenue'].describe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287539425
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall Segmentation\r\n",
        "\r\n",
        "Awesome! We have scores (cluster numbers) for recency, frequency & revenue. Let’s create an overall score out of them:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287540001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate overall score and use mean() to see details\r\n",
        "\r\n",
        "tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']\r\n",
        "tx_user"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287540616
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287541213
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The scoring above clearly shows us that customers with score 8 is our best customers whereas 0 is the worst."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user.groupby('OverallScore')['Recency'].count()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287541821
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple, better we name these scores:\r\n",
        "- 0 to 2: Low Value\r\n",
        "- 3 to 4: Mid Value\r\n",
        "- 5+: High Value\r\n",
        "\r\n",
        "We can easily apply this naming on our dataframe:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_user['Segment'] = 'Low-Value'\n",
        "tx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' \n",
        "tx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value' \n",
        "\n",
        "tx_user"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287542409
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, it is the best part. Let’s see how our segments distributed on a scatter plot:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_graph = tx_user.query(\"Revenue < 50000 and Frequency < 2000\")\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='Mid',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"Revenue\"},\n",
        "        xaxis= {'title': \"Frequency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287543053
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_graph = tx_user.query(\"Revenue < 50000 and Frequency < 2000\")\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='Mid',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['Revenue'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"Revenue\"},\n",
        "        xaxis= {'title': \"Recency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287544036
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tx_graph = tx_user.query(\"Revenue < 50000 and Frequency < 2000\")\n",
        "\n",
        "plot_data = [\n",
        "    go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Low-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Low-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='Low',\n",
        "        marker= dict(size= 7,\n",
        "            line= dict(width=1),\n",
        "            color= 'blue',\n",
        "            opacity= 0.8\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'Mid-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'Mid-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='Mid',\n",
        "        marker= dict(size= 9,\n",
        "            line= dict(width=1),\n",
        "            color= 'green',\n",
        "            opacity= 0.5\n",
        "           )\n",
        "    ),\n",
        "        go.Scatter(\n",
        "        x=tx_graph.query(\"Segment == 'High-Value'\")['Recency'],\n",
        "        y=tx_graph.query(\"Segment == 'High-Value'\")['Frequency'],\n",
        "        mode='markers',\n",
        "        name='High',\n",
        "        marker= dict(size= 11,\n",
        "            line= dict(width=1),\n",
        "            color= 'red',\n",
        "            opacity= 0.9\n",
        "           )\n",
        "    ),\n",
        "]\n",
        "\n",
        "plot_layout = go.Layout(\n",
        "        yaxis= {'title': \"Frequency\"},\n",
        "        xaxis= {'title': \"Recency\"},\n",
        "        title='Segments'\n",
        "    )\n",
        "fig = go.Figure(data=plot_data, layout=plot_layout)\n",
        "pyoff.iplot(fig)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1631287544647
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can start taking actions with this segmentation. The main strategies are quite clear:\r\n",
        "- High Value: Improve Retention\r\n",
        "- Mid Value: Improve Retention + Increase Frequency\r\n",
        "- Low Value: Increase Frequency*"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}