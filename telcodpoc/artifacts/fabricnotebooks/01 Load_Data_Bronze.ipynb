{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919030ec-a347-4c96-9b92-7ada89e18c9b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T11:05:30.5491362Z",
       "execution_start_time": "2025-05-14T11:05:30.1653011Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "cc850800-9254-4c59-969f-d0249bc56f62",
       "queued_time": "2025-05-14T11:05:19.1725074Z",
       "session_id": "b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1",
       "session_start_time": "2025-05-14T11:05:19.1735966Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import shutil\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "spark.conf.set(\"spark.sql.caseSensitive\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccecee7-12fb-4b9a-a689-453115c7da00",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T11:05:30.8584525Z",
       "execution_start_time": "2025-05-14T11:05:30.5514868Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7a1703b2-aaa6-4319-829c-861370aaec93",
       "queued_time": "2025-05-14T11:05:19.1748795Z",
       "session_id": "b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Source_LAKEHOUSE_Operations_NAME=\"abfss://#wsid#@onelake.dfs.fabric.microsoft.com/#Lakehouse_Bronze_ID#/Files/operation_datastore_S3\"\n",
    "Source_LAKEHOUSE_Finance_NAME=\"abfss://#wsid#@onelake.dfs.fabric.microsoft.com/#Lakehouse_Bronze_ID#/Files/finance_datastore_gcp\"\n",
    "Source_LAKEHOUSE_Network_NAME=\"abfss://#wsid#@onelake.dfs.fabric.microsoft.com/#Lakehouse_Bronze_ID#/Files/ network_datastore_adls\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f5f6f-44dc-4879-b98f-bf878811376d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load_Operation_Files_to_Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b54882-dda7-4205-b7bb-5e5a1e4f5e4e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T11:06:27.497958Z",
       "execution_start_time": "2025-05-14T11:05:30.8606467Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "74315d63-19c6-443a-9151-1b4b0626a007",
       "queued_time": "2025-05-14T11:05:19.1768903Z",
       "session_id": "b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'CommunicationDevice'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Device'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Employee'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Issue'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'IssueActivity'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'IssueStatus'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'IssueType'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# abfss path to the folder where the files are located\n",
    "file_path = \"abfss://#wsid#@onelake.dfs.fabric.microsoft.com/#Lakehouse_Bronze_ID#/Files/operation_datastore_S3/\"\n",
    "\n",
    "# the mssparkutils.fs.ls method lists all the files (and/or subfolders) inside the folder\n",
    "files = mssparkutils.fs.ls(file_path)\n",
    "\n",
    "# Convert FileInfo objects to list of tuples (this creates a list of the file names in the folder)\n",
    "file_data = [(file.name,) for file in files]\n",
    "\n",
    "# This creates a dataframe consisting of the file names in the folder\n",
    "df_files_in_folder = spark.createDataFrame(file_data, [\"name\"])\n",
    "\n",
    "# Show the DataFrame (this step can be removed)\n",
    "#display(df_files_in_folder)\n",
    "\n",
    "\n",
    "# Iterate through the filesa\n",
    "for file in df_files_in_folder.rdd.collect(): # Collecting to driver (local) as a list of Rows \n",
    "\n",
    "    # Extract filename from the current row\n",
    "    filename = file[\"name\"]\n",
    "\n",
    "    tablename = filename[:-4]\n",
    "\n",
    "    display(tablename)\n",
    "\n",
    "    target_schema = spark.table(tablename).schema\n",
    "\n",
    "    # Read the current file into a dataframe\n",
    "    df_src = spark.read.format(\"csv\").option(\"header\", \"true\").schema(target_schema).load(file_path + filename)\n",
    "\n",
    "    # Append the dataframe to the table\n",
    "    df_src.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tablename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41c934-a2e6-4672-8721-c5cbb2cf4f85",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Load_Network_Files_to_Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e960385-73c8-4f9b-b91f-79984841918d",
   "metadata": {
    "advisor": {
     "adviceMetadata": "{\"artifactId\":\"66a41b14-4e5a-4be2-954a-544e0f035b7c\",\"activityId\":\"b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1\",\"applicationId\":\"application_1747220285862_0001\",\"jobGroupId\":\"6\",\"advices\":{\"info\":2}}"
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-05-14T11:07:10.4352028Z",
       "execution_start_time": "2025-05-14T11:06:27.5007883Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "5eced2c8-fa87-40ff-9f05-9bd9a5114a6a",
       "queued_time": "2025-05-14T11:05:19.1787553Z",
       "session_id": "b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, b27ad5a1-dbb9-4c4d-9161-c0de5f0350f1, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BaseStation'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'BaseStationMetrics'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Network'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NetworkEvent'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NetworkOccupiedBandwidth'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NetworkServiceArea'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NetworkTransaction'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'UnplannedDisruption'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# abfss path to the folder where the files are located\n",
    "file_path = \"abfss://#wsid#@onelake.dfs.fabric.microsoft.com/#Lakehouse_Bronze_ID#/Files/ network_datastore_adls/\"\n",
    "\n",
    "# the mssparkutils.fs.ls method lists all the files (and/or subfolders) inside the folder\n",
    "files = mssparkutils.fs.ls(file_path)\n",
    "\n",
    "# Convert FileInfo objects to list of tuples (this creates a list of the file names in the folder)\n",
    "file_data = [(file.name,) for file in files]\n",
    "\n",
    "# This creates a dataframe consisting of the file names in the folder\n",
    "df_files_in_folder = spark.createDataFrame(file_data, [\"name\"])\n",
    "\n",
    "# Show the DataFrame (this step can be removed)\n",
    "#display(df_files_in_folder)\n",
    "\n",
    "\n",
    "# Iterate through the filesa\n",
    "for file in df_files_in_folder.rdd.collect(): # Collecting to driver (local) as a list of Rows \n",
    "\n",
    "    # Extract filename from the current row\n",
    "    filename = file[\"name\"]\n",
    "\n",
    "    tablename = filename[:-4]\n",
    "\n",
    "    display(tablename)\n",
    "\n",
    "    target_schema = spark.table(tablename).schema\n",
    "\n",
    "    # Read the current file into a dataframe\n",
    "    df_src = spark.read.format(\"csv\").option(\"header\", \"true\").schema(target_schema).load(file_path + filename)\n",
    "\n",
    "    # Append the dataframe to the table\n",
    "    df_src.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tablename)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "e6200c53-d1f0-43ce-af32-21ee0f1f49de",
    "default_lakehouse_name": "Lakehouse_Bronze_lnp0dx1",
    "default_lakehouse_workspace_id": "7a30a8eb-d049-42fb-93e2-31e651025b70",
    "known_lakehouses": [
     {
      "id": "e6200c53-d1f0-43ce-af32-21ee0f1f49de"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
