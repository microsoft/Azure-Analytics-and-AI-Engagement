{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Relevant Libraries\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "!pip install --use-feature=2020-resolver azure-storage-blob==2.1.0\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from azure.storage.blob import BlockBlobService\n",
        "import pprint\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "import pickle"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Local Folders\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create local directories if they don't exist\n",
        "# *mfg_source* contains all the pdf files to be converted to json\n",
        "if (not os.path.isdir(os.getcwd()+\"/form-datasets\")):\n",
        "    os.makedirs(os.getcwd()+\"/form-datasets\")\n",
        "# *formrecogoutput* will contain all the converted json files\n",
        "if (not os.path.isdir(os.getcwd()+\"/formrecogoutput\")):\n",
        "    os.makedirs(os.getcwd()+\"/formrecogoutput\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the PDF form from a cotainer\n",
        "\n",
        "Downloads all PDF from a container\n",
        "Does not download PDF which are already downloaded"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%time\n",
        "# Downloading pdf files from a container named *form-datasets* to a local folder *form-datasets**\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "STORAGE_CONTAINER_NAME = \"form-datasets\"\n",
        "\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY) \n",
        "\n",
        "blobs = blob_service.list_blobs(STORAGE_CONTAINER_NAME)\n",
        "# Downloading pdf files from the container *form-datasets** and storing them locally to *form-datasets** folder\n",
        "for blob in blobs:\n",
        "    # Check if the blob.name is already present in the folder form-datasets*. If yes then continue\n",
        "    try:\n",
        "        with open('merged_log','rb') as f:\n",
        "            merged_files = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        merged_files = set()\n",
        "    # If file is already processed then continue to next file\n",
        "    if (blob.name in merged_files): \n",
        "        continue\n",
        "    download_file_path = os.path.join(os.getcwd(), \"form-datasets\", blob.name)\n",
        "    blob_service.get_blob_to_path(STORAGE_CONTAINER_NAME, blob.name ,download_file_path)\n",
        "    merged_files.add(blob.name)\n",
        "    # Keep trace of all the processed files at the end of your script (to keep track later)\n",
        "    with open('merged_log', 'wb') as f:\n",
        "        pickle.dump(merged_files, f)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Total number of forms to be converted to JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/form-datasets\") if isfile(join(os.getcwd()+\"/form-datasets\", f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Querying the custom form recognizer model (PDF -> JSON)\n",
        "ï²\n",
        "Converts PDF -> JSON by querying the trained custom model.\n",
        "Clean the JSON file\n",
        "If a file has already been converted to JSON then skip it.## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%time\n",
        "# Endpoint parameters for querying the custom trained form-recognizer model to return the processed JSON\n",
        "# Processes PDF files one by one and return CLEAN JSON files\n",
        "endpoint = r\"https://westus2.api.cognitive.microsoft.com/\"\n",
        "# Change if api key is expired\n",
        "apim_key = \"1d6a7980b0a84bc9b040a1196436675e\"\n",
        "# This model is the one trained on 5 forms\n",
        "model_id = \"0986b966-c6ae-4b9c-8eec-4db9bb846eac\"\n",
        "post_url = endpoint + r\"/formrecognizer/v2.0/custom/models/%s/analyze\" % model_id\n",
        "files = [f for f in listdir(os.getcwd()+\"/form-datasets\") if isfile(join(os.getcwd()+\"/form-datasets\", f))]\n",
        "params = {\"includeTextDetails\": True}\n",
        "headers = {'Content-Type': 'application/pdf', 'Ocp-Apim-Subscription-Key': apim_key}\n",
        "\n",
        "local_path = os.path.join(os.getcwd(), \"form-datasets//\")\n",
        "output_path = os.path.join(os.getcwd(), \"formrecogoutput//\")\n",
        "\n",
        "for file in files:\n",
        "    try:\n",
        "        with open('json_log','rb') as l:\n",
        "            json_files = pickle.load(l)\n",
        "    except FileNotFoundError:\n",
        "        json_files = set()\n",
        "    if (file in json_files): \n",
        "        continue\n",
        "    else:\n",
        "        with open(local_path+file, \"rb\") as f:\n",
        "            data_bytes = f.read()\n",
        "        \n",
        "    try:\n",
        "        resp = requests.post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
        "        print('resp',resp)\n",
        "        if resp.status_code != 202:\n",
        "            print(\"POST analyze failed:\\n%s\" % json.dumps(resp.json()))\n",
        "            quit()\n",
        "        print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
        "        get_url = resp.headers[\"operation-location\"]\n",
        "    except Exception as e:\n",
        "        print(\"POST analyze failed:\\n%s\" % str(e))\n",
        "        quit()\n",
        "     \n",
        "    n_tries = 15\n",
        "    n_try = 0\n",
        "    wait_sec = 5\n",
        "    max_wait_sec = 60\n",
        "    while n_try < n_tries:\n",
        "        try:\n",
        "            resp = requests.get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": apim_key})\n",
        "            resp_json = resp.json()\n",
        "            if resp.status_code != 200:\n",
        "                print(\"GET analyze results failed:\\n%s\" % json.dumps(resp_json))\n",
        "                quit()\n",
        "            status = resp_json[\"status\"]\n",
        "            if status == \"succeeded\":\n",
        "                print(\"Analysis succeeded:\\n%s\" % file[:-4])\n",
        "                allkeys = resp_json['analyzeResult']['documentResults'][0]['fields'].keys()\n",
        "                new_dict = {}\n",
        "                for i in allkeys:\n",
        "                    if resp_json['analyzeResult']['documentResults'][0]['fields'][i] != None:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = resp_json['analyzeResult']['documentResults'][0]['fields'][i]['valueString']\n",
        "                    else:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = None\n",
        "                # Appending form url to json\n",
        "                new_dict['form_url'] = 'https://dreamdemostrggen2pocrs11.blob.core.windows.net/formupload/' + file \n",
        "                with open(output_path+file[:-4]+\".json\", 'w') as outfile:\n",
        "                    json.dump(new_dict, outfile)\n",
        "                # Change the encoding of file in case of spanish forms. It will detected random characters\n",
        "                with open(output_path+file[:-4]+\".json\", 'w', encoding='utf-8') as outfile:\n",
        "                    json.dump(new_dict, outfile, ensure_ascii=False)\n",
        "                # Once JSON is saved log it otherwise don't log it.\n",
        "                json_files.add(file)\n",
        "                with open('json_log', 'wb') as f:\n",
        "                    pickle.dump(json_files, f)\n",
        "\n",
        "                break\n",
        "            if status == \"failed\":\n",
        "                print(\"Analysis failed:\\n%s\" % json.dumps(resp_json))\n",
        "                quit()\n",
        "            # Analysis still running. Wait and retry.\n",
        "            time.sleep(wait_sec)\n",
        "            n_try += 1\n",
        "            wait_sec = min(2*wait_sec, max_wait_sec)     \n",
        "        except Exception as e:\n",
        "            msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "            print(msg)\n",
        "            quit()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the JSON files to a cotainer## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Total number of converted JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/formrecogoutput\") if isfile(join(os.getcwd()+\"/formrecogoutput\", f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%time\n",
        "# Connect to the container for uploading the JSON files\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "# Upload the JSON files in this container\n",
        "STORAGE_CONTAINER_NAME = \"formrecogoutput\"\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%time\n",
        "# Upload JSON files from local folder *formrecogoutput* to the container *formrecogoutput*\n",
        "local_path = os.path.join(os.getcwd(), \"formrecogoutput\")\n",
        "print(local_path)\n",
        "for files in os.listdir(local_path):\n",
        "    print(os.path.join(local_path,files))\n",
        "    blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, files, os.path.join(local_path,files))"
      ],
      "attachments": {}
    }
  ]
}