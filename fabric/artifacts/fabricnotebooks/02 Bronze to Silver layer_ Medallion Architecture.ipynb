{"cells":[{"cell_type":"markdown","id":"4e108de0-da8d-4f16-bfe4-9c4d0cfa178c","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Transform data from Bronze 'Raw data' to Silver 'Cleansed and conformed data' layer and perform data transformation"]},{"cell_type":"markdown","id":"2f7df7c0-4e24-45cc-ab93-4e55c1bdeb6d","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["![Medallion Architecture](https://fabricddib.blob.core.windows.net/notebookimage/MedallionArchitecture.png)"]},{"cell_type":"markdown","id":"38e9203b-8af2-4870-86dd-4b1e3acec601","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Importing the necessary libraries and spark configurations."]},{"cell_type":"code","execution_count":1,"id":"7a5c0ecf-6c11-4caa-83cb-ffc12dde0fd9","metadata":{},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"parent_msg_id":"17aabbf8-912d-4d1f-98e9-730ef76f276f","queued_time":"2023-09-11T22:28:05.1991945Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import col, unix_timestamp, to_date,col,year,quarter,month\n","from pyspark.sql.types import DateType\n","from pyspark.sql.functions import col, unix_timestamp, to_date,col,year,quarter,month\n","from pyspark.sql.functions import col\n","from pyspark.sql.types import IntegerType\n","from pyspark.sql.types import DoubleType\n","from pyspark.sql.types import DateType\n","from pyspark.sql import functions as F"]},{"cell_type":"markdown","id":"6c25d7f2-7474-4623-9266-f466f5d8f667","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Set input parameters"]},{"cell_type":"code","execution_count":29,"id":"e108c017-9863-48de-a37b-dc9a258ae7b1","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"parent_msg_id":"9d030883-35d0-46ce-9e2c-06e9c1f06a74","queued_time":"2023-09-11T22:28:05.2076977Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["Path_Dim='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/DimensionData/'\n","Path_Fact='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/FactData/'\n","Path_LitwareData='abfss://#SALES_WORKSPACE_NAME#@onelake.dfs.fabric.microsoft.com/#LAKEHOUSE_BRONZE#.Lakehouse/Files/sales-transaction-litware'\n"]},{"cell_type":"markdown","id":"491bf3da-5ae1-4d35-999f-2e4f02c258ce","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension - Brand\n","We created a shortcut to raw data that we landed earlier in the Bronze lakehouse. We then do the necessary cleanup and transformation on the data and write the dimension table to the silver lakehouse in open standard delta parquet format. The table starts appearing under the tables pane as soon as we execute this cell. We follow the similar approach for rest of the tables."]},{"cell_type":"code","execution_count":3,"id":"92d1f2c9-85f0-40bd-8c79-feabf86760d4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:30.7351905Z","execution_start_time":"2023-09-11T18:46:13.9692377Z","livy_statement_state":"available","parent_msg_id":"b0b469e5-8af6-4efa-aed0-b95abce783cd","queued_time":"2023-09-11T18:46:13.6394284Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:29.135GMT","dataRead":4344,"dataWritten":0,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"5","jobId":16,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[27,28,26],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:29.056GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:29.032GMT","dataRead":4931,"dataWritten":4344,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"5","jobId":15,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[24,25],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:28.257GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:28.064GMT","dataRead":3730,"dataWritten":4931,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"5","jobId":14,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[23],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:27.936GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:27.346GMT","dataRead":1753,"dataWritten":0,"description":"Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"5","jobId":13,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":3,"stageIds":[21,22],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:27.134GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:27.026GMT","dataRead":307,"dataWritten":1817,"description":"Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"5","jobId":12,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":14,"stageIds":[19,20],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:26.669GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:26.624GMT","dataRead":205,"dataWritten":307,"description":"Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"5","jobId":11,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":14,"stageIds":[18],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:26.425GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:26.037GMT","dataRead":4339,"dataWritten":0,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"5","jobId":10,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[15,16,17],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:25.981GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:25.956GMT","dataRead":3229,"dataWritten":4339,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"5","jobId":9,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[13,14],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:25.167GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:24.988GMT","dataRead":2556,"dataWritten":3229,"description":"Delta: Job group for statement 5:\ntable_name = 'dimension_brand'\ndimension_campaignchatgpt_schema = StructType([\n    StructField('BrandId', IntegerType(), True),\n    StructField('BrandName', StringType(), True),\n    StructField('EntityCode', StringType(), True)]\n)\ndf = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"5","jobId":8,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[12],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:24.678GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":5},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 5, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_brand'\n","dimension_campaignchatgpt_schema = StructType([\n","    StructField('BrandId', IntegerType(), True),\n","    StructField('BrandName', StringType(), True),\n","    StructField('EntityCode', StringType(), True)]\n",")\n","df = spark.read.format(\"csv\").schema(dimension_campaignchatgpt_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"c522ba39-1f0b-4ba7-b3d0-6f5f5f2303fb","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Campaign"]},{"cell_type":"code","execution_count":4,"id":"bfb88cdb-57ed-4be6-89fb-332cff83c369","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:36.0877088Z","execution_start_time":"2023-09-11T18:46:31.1519691Z","livy_statement_state":"available","parent_msg_id":"669d53dd-6fca-4a41-a948-099ca761f789","queued_time":"2023-09-11T18:46:13.9189799Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:35.166GMT","dataRead":4344,"dataWritten":0,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"6","jobId":25,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[45,43,44],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:35.100GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:35.072GMT","dataRead":4834,"dataWritten":4344,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"6","jobId":24,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[42,41],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:34.420GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:34.305GMT","dataRead":3788,"dataWritten":4834,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"6","jobId":23,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[40],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:34.225GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:33.739GMT","dataRead":1743,"dataWritten":0,"description":"Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"6","jobId":22,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":3,"stageIds":[38,39],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:33.577GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:33.431GMT","dataRead":298,"dataWritten":1988,"description":"Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"6","jobId":21,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":14,"stageIds":[37,36],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:33.116GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:33.078GMT","dataRead":221,"dataWritten":298,"description":"Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"6","jobId":20,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":14,"stageIds":[35],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:33.000GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:32.761GMT","dataRead":4339,"dataWritten":0,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"6","jobId":19,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[33,34,32],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:32.708GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:32.688GMT","dataRead":3130,"dataWritten":4339,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"6","jobId":18,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[30,31],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:32.031GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:31.821GMT","dataRead":2576,"dataWritten":3130,"description":"Delta: Job group for statement 6:\ntable_name = 'dimension_campaign'\ndimension_campaign_schema = StructType([\n    StructField('Campaigns_ID', IntegerType(), True), \n    StructField('CampaignName', StringType(), True), \n    StructField('SubCampaignId', StringType(), True)] \n)\ndf = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"6","jobId":17,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[29],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:31.716GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":6},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 6, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_campaign'\n","dimension_campaign_schema = StructType([\n","    StructField('Campaigns_ID', IntegerType(), True), \n","    StructField('CampaignName', StringType(), True), \n","    StructField('SubCampaignId', StringType(), True)] \n",")\n","df = spark.read.format(\"csv\").schema(dimension_campaign_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df = df.withColumnRenamed(\"Campaigns_ID\", \"CampaignId\")\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"d6c63b1b-13dd-4e13-b163-02c085780347","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Customer"]},{"cell_type":"code","execution_count":5,"id":"8bb88200-7051-40ed-8ba5-a1357b36e0a2","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:41.3722353Z","execution_start_time":"2023-09-11T18:46:36.4959518Z","livy_statement_state":"available","parent_msg_id":"3a90aadd-e10f-4d51-b0db-cfe3ef4f99c0","queued_time":"2023-09-11T18:46:14.3761114Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:40.408GMT","dataRead":4426,"dataWritten":0,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"7","jobId":34,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":54,"numTasks":55,"rowCount":50,"stageIds":[60,61,62],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:40.364GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:40.348GMT","dataRead":6710,"dataWritten":4426,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"7","jobId":33,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":61,"stageIds":[58,59],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:39.783GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:39.660GMT","dataRead":6649,"dataWritten":6710,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"7","jobId":32,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":4,"numCompletedStages":1,"numCompletedTasks":4,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":4,"rowCount":22,"stageIds":[57],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:39.569GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:39.126GMT","dataRead":2187,"dataWritten":0,"description":"Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"7","jobId":31,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":3,"stageIds":[56,55],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:38.900GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:38.812GMT","dataRead":763267,"dataWritten":572032,"description":"Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"7","jobId":30,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":10000,"stageIds":[53,54],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:38.381GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:38.331GMT","dataRead":820388,"dataWritten":763267,"description":"Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"7","jobId":29,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":10000,"stageIds":[52],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:38.081GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:37.837GMT","dataRead":4421,"dataWritten":0,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"7","jobId":28,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[51,49,50],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:37.801GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:37.780GMT","dataRead":4646,"dataWritten":4421,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"7","jobId":27,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":58,"stageIds":[48,47],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:37.259GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:37.115GMT","dataRead":4859,"dataWritten":4646,"description":"Delta: Job group for statement 7:\ntable_name = 'dimension_customer'\ndimension_customer_schema = StructType([\n    StructField('Id', IntegerType(), True), \n    StructField('Age', IntegerType(), True), \n    StructField('Gender', StringType(), True), \n    StructField('Pincode', StringType(), True), \n    StructField('FirstName', StringType(), True), \n    StructField('LastName', StringType(), True), \n    StructField('FullName', StringType(), True), \n    StructField('DateOfBirth', StringType(), True), \n    StructField('Address', StringType(), True), \n    StructField('Email', StringType(), True), \n    StructField('Mobile', StringType(), True),\n    StructField('UserName', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"7","jobId":26,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":16,"stageIds":[46],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:37.037GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":7},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 7, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_customer'\n","dimension_customer_schema = StructType([\n","    StructField('Id', IntegerType(), True), \n","    StructField('Age', IntegerType(), True), \n","    StructField('Gender', StringType(), True), \n","    StructField('Pincode', StringType(), True), \n","    StructField('FirstName', StringType(), True), \n","    StructField('LastName', StringType(), True), \n","    StructField('FullName', StringType(), True), \n","    StructField('DateOfBirth', StringType(), True), \n","    StructField('Address', StringType(), True), \n","    StructField('Email', StringType(), True), \n","    StructField('Mobile', StringType(), True),\n","    StructField('UserName', StringType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_customer_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"fdb5f422-22e2-427e-8974-19af5aba7f90","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Date"]},{"cell_type":"code","execution_count":6,"id":"0e89e399-8525-420f-9079-278cbdeae04e","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:46.6476207Z","execution_start_time":"2023-09-11T18:46:41.7709557Z","livy_statement_state":"available","parent_msg_id":"e1f2e0ae-b051-4a71-84c3-12fe9d3e1a5d","queued_time":"2023-09-11T18:46:15.0777909Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:45.178GMT","dataRead":4425,"dataWritten":0,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"8","jobId":43,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[78,79,77],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:45.146GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:45.132GMT","dataRead":4359,"dataWritten":4425,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"8","jobId":42,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":58,"stageIds":[75,76],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:44.646GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:44.539GMT","dataRead":4509,"dataWritten":4359,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"8","jobId":41,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":16,"stageIds":[74],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:44.478GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:44.030GMT","dataRead":1476,"dataWritten":0,"description":"Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"8","jobId":40,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":2,"stageIds":[72,73],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:43.891GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:43.804GMT","dataRead":31557,"dataWritten":22160,"description":"Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"8","jobId":39,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":2190,"stageIds":[70,71],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:43.466GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:43.423GMT","dataRead":99538,"dataWritten":31557,"description":"Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"8","jobId":38,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":2190,"stageIds":[69],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:43.302GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:43.067GMT","dataRead":4412,"dataWritten":0,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"8","jobId":37,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[66,67,68],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:43.040GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:43.023GMT","dataRead":2878,"dataWritten":4412,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"8","jobId":36,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":56,"stageIds":[64,65],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:42.519GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:42.319GMT","dataRead":3098,"dataWritten":2878,"description":"Delta: Job group for statement 8:\ntable_name = 'dimension_date'\ndimension_date_schema = StructType([\n    StructField('DateKey', IntegerType(), True), \n    StructField('DateValue', TimestampType(), True), \n    StructField('DayOfMonth', IntegerType(), True), \n    StructField('DayOfYear', IntegerType(), True), \n    StructField('Year', IntegerType(), True), \n    StructField('MonthOfYear', IntegerType(), True), \n    StructField('MonthName', StringType(), True), \n    StructField('QuarterOfYear', IntegerType(), True), \n    StructField('QuarterName', StringType(), True), \n    StructField('WeekEnding', TimestampType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"8","jobId":35,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":12,"stageIds":[63],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:42.267GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":8},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 8, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_date'\n","dimension_date_schema = StructType([\n","    StructField('DateKey', IntegerType(), True), \n","    StructField('DateValue', TimestampType(), True), \n","    StructField('DayOfMonth', IntegerType(), True), \n","    StructField('DayOfYear', IntegerType(), True), \n","    StructField('Year', IntegerType(), True), \n","    StructField('MonthOfYear', IntegerType(), True), \n","    StructField('MonthName', StringType(), True), \n","    StructField('QuarterOfYear', IntegerType(), True), \n","    StructField('QuarterName', StringType(), True), \n","    StructField('WeekEnding', TimestampType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_date_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"bef34297-a7f7-411f-8eb6-9c944144a489","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Country"]},{"cell_type":"code","execution_count":7,"id":"1842727e-aaa0-4956-a4f3-692005c65624","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:52.0465571Z","execution_start_time":"2023-09-11T18:46:47.0884743Z","livy_statement_state":"available","parent_msg_id":"a3e4505e-1a69-4aac-bf6b-8d5917364703","queued_time":"2023-09-11T18:46:15.8053987Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:50.159GMT","dataRead":4336,"dataWritten":0,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"9","jobId":52,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":54,"numTasks":55,"rowCount":50,"stageIds":[96,94,95],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:50.132GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:50.116GMT","dataRead":6460,"dataWritten":4336,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"9","jobId":51,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":54,"rowCount":63,"stageIds":[93,92],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:49.683GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:49.479GMT","dataRead":4691,"dataWritten":6460,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 3","displayName":"toString at String.java:2994","jobGroup":"9","jobId":50,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":4,"numCompletedStages":1,"numCompletedTasks":4,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":4,"rowCount":26,"stageIds":[91],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:49.415GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:48.997GMT","dataRead":1732,"dataWritten":0,"description":"Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"9","jobId":49,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":3,"stageIds":[89,90],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:48.879GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:48.804GMT","dataRead":634,"dataWritten":1960,"description":"Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"9","jobId":48,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":68,"stageIds":[88,87],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:48.574GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:48.536GMT","dataRead":704,"dataWritten":634,"description":"Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"9","jobId":47,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":68,"stageIds":[86],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:48.479GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:48.272GMT","dataRead":4331,"dataWritten":0,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"9","jobId":46,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[84,85,83],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:48.246GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:48.221GMT","dataRead":4772,"dataWritten":4331,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"9","jobId":45,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[81,82],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:47.768GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:47.649GMT","dataRead":3551,"dataWritten":4772,"description":"Delta: Job group for statement 9:\ntable_name = 'dimension_country'\ndimension_country_schema = StructType([\n    StructField('ID', IntegerType(), True),\n    StructField('Country', StringType(), True), \n    StructField('Region', StringType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"9","jobId":44,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[80],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:47.592GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":9},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 9, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_country'\n","dimension_country_schema = StructType([\n","    StructField('ID', IntegerType(), True),\n","    StructField('Country', StringType(), True), \n","    StructField('Region', StringType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_country_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"a1a089b2-97a1-4faf-8742-091576d76e79","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-City"]},{"cell_type":"code","execution_count":8,"id":"fd3b1d4a-1e40-4db1-9a23-67912a3f82d9","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:46:58.881065Z","execution_start_time":"2023-09-11T18:46:52.4457885Z","livy_statement_state":"available","parent_msg_id":"84825c88-d503-4b88-8298-aa6a7c8e988a","queued_time":"2023-09-11T18:46:16.2087048Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:46:56.855GMT","dataRead":4479,"dataWritten":0,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"10","jobId":61,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[111,112,113],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:56.829GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:56.812GMT","dataRead":3850,"dataWritten":4479,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"10","jobId":60,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[109,110],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:56.433GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:56.309GMT","dataRead":5105,"dataWritten":3850,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"10","jobId":59,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[108],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:56.253GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:55.874GMT","dataRead":2268,"dataWritten":0,"description":"Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"10","jobId":58,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[107,106],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:55.760GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:55.650GMT","dataRead":6317148,"dataWritten":2401969,"description":"Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"10","jobId":57,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":6,"rowCount":232590,"stageIds":[104,105],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:54.604GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:54.548GMT","dataRead":20587379,"dataWritten":6317148,"description":"Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"10","jobId":56,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":5,"numCompletedStages":1,"numCompletedTasks":5,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":5,"rowCount":232590,"stageIds":[103],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:53.910GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:53.642GMT","dataRead":4474,"dataWritten":0,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"10","jobId":55,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[102,100,101],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:53.613GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:53.589GMT","dataRead":2055,"dataWritten":4474,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"10","jobId":54,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[99,98],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:53.158GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:53.052GMT","dataRead":3095,"dataWritten":2055,"description":"Delta: Job group for statement 10:\ntable_name = 'dimension_city'\ndimension_city_schema = StructType([\n    StructField('CityKey', IntegerType(), True),\n    StructField('CityID', IntegerType(), True), \n    StructField('City', StringType(), True),\n    StructField('StateProvince', StringType(), True),\n    StructField('Country', StringType(), True),\n    StructField('Continent', StringType(), True),\n    StructField('SalesTerritory', StringType(), True),\n    StructField('Region', StringType(), True),\n    StructField('SubRegion', StringType(), True),\n    StructField('Location', StringType(), True),\n    StructField('LatestRecordedPopulation', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"10","jobId":53,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[97],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:53.003GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":10},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 10, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_city'\n","dimension_city_schema = StructType([\n","    StructField('CityKey', IntegerType(), True),\n","    StructField('CityID', IntegerType(), True), \n","    StructField('City', StringType(), True),\n","    StructField('StateProvince', StringType(), True),\n","    StructField('Country', StringType(), True),\n","    StructField('Continent', StringType(), True),\n","    StructField('SalesTerritory', StringType(), True),\n","    StructField('Region', StringType(), True),\n","    StructField('SubRegion', StringType(), True),\n","    StructField('Location', StringType(), True),\n","    StructField('LatestRecordedPopulation', StringType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTo', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"d155bdcb-c674-4557-b045-cb772a07c509","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Employee"]},{"cell_type":"code","execution_count":9,"id":"dde7f2e4-68c3-460c-94c2-6f807f747881","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:04.2205959Z","execution_start_time":"2023-09-11T18:46:59.2978569Z","livy_statement_state":"available","parent_msg_id":"83949565-b474-48ad-a239-f69c684663dd","queued_time":"2023-09-11T18:46:16.6806078Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:02.271GMT","dataRead":4402,"dataWritten":0,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"11","jobId":70,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[129,130,128],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:02.241GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:02.221GMT","dataRead":3583,"dataWritten":4402,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"11","jobId":69,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[126,127],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:01.802GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:01.686GMT","dataRead":3730,"dataWritten":3583,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"11","jobId":68,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[125],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:01.633GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:01.250GMT","dataRead":1947,"dataWritten":0,"description":"Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"11","jobId":67,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[123,124],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:01.156GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:01.057GMT","dataRead":7133,"dataWritten":8467,"description":"Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"11","jobId":66,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":426,"stageIds":[121,122],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:00.801GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:00.765GMT","dataRead":16291,"dataWritten":7133,"description":"Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"11","jobId":65,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":426,"stageIds":[120],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:00.700GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:00.485GMT","dataRead":4397,"dataWritten":0,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"11","jobId":64,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[117,118,119],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:00.462GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:00.448GMT","dataRead":1737,"dataWritten":4397,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"11","jobId":63,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[115,116],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:59.976GMT","usageDescription":""},{"completionTime":"2023-09-11T18:46:59.861GMT","dataRead":2175,"dataWritten":1737,"description":"Delta: Job group for statement 11:\ntable_name = 'dimension_employee'\ndimension_city_schema = StructType([\n    StructField('EmployeeKey', IntegerType(), True),\n    StructField('EmployeeID', IntegerType(), True), \n    StructField('EmployeeName', StringType(), True),\n    StructField('PreferredName', StringType(), True),\n    StructField('IsSalesPerson', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"11","jobId":62,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[114],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:46:59.815GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":11},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 11, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_employee'\n","dimension_city_schema = StructType([\n","    StructField('EmployeeKey', IntegerType(), True),\n","    StructField('EmployeeID', IntegerType(), True), \n","    StructField('EmployeeName', StringType(), True),\n","    StructField('PreferredName', StringType(), True),\n","    StructField('IsSalesPerson', StringType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTo', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"594540db-9e34-4856-a53f-03173a6deeb7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Paymentmethod"]},{"cell_type":"code","execution_count":10,"id":"b4ffff06-f6e1-4fea-9e69-d34b5878053b","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:09.5475202Z","execution_start_time":"2023-09-11T18:47:04.6426125Z","livy_statement_state":"available","parent_msg_id":"9bdd5be2-78b0-4b8b-aafd-d85004e23eea","queued_time":"2023-09-11T18:46:17.2281324Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:07.609GMT","dataRead":4373,"dataWritten":0,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"12","jobId":79,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[147,145,146],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:07.586GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:07.569GMT","dataRead":3426,"dataWritten":4373,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"12","jobId":78,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[143,144],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:07.169GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:07.071GMT","dataRead":3315,"dataWritten":3426,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"12","jobId":77,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[142],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:06.994GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:06.602GMT","dataRead":1860,"dataWritten":0,"description":"Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"12","jobId":76,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[140,141],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:06.499GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:06.420GMT","dataRead":342,"dataWritten":3351,"description":"Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"12","jobId":75,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":12,"stageIds":[139,138],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:06.143GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:06.103GMT","dataRead":441,"dataWritten":342,"description":"Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"12","jobId":74,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":12,"stageIds":[137],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:06.054GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:05.845GMT","dataRead":4368,"dataWritten":0,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"12","jobId":73,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[135,136,134],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:05.816GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:05.803GMT","dataRead":1641,"dataWritten":4368,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"12","jobId":72,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[132,133],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:05.390GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:05.249GMT","dataRead":1890,"dataWritten":1641,"description":"Delta: Job group for statement 12:\ntable_name = 'dimension_paymentmethod'\ndimension_city_schema = StructType([\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('PaymentMethodID', IntegerType(), True), \n    StructField('PaymentMethod', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTo', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\" + table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"12","jobId":71,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[131],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:05.180GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":12},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 12, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_paymentmethod'\n","dimension_city_schema = StructType([\n","    StructField('PaymentMethodKey', IntegerType(), True),\n","    StructField('PaymentMethodID', IntegerType(), True), \n","    StructField('PaymentMethod', StringType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTo', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\" + table_name)"]},{"cell_type":"markdown","id":"31db619d-7dbe-4cbf-830c-d73fa7dec7d6","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Supplier"]},{"cell_type":"code","execution_count":11,"id":"312c42ed-176d-46cd-b22d-21a3d784df50","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"13\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:14.8707622Z","execution_start_time":"2023-09-11T18:47:09.934531Z","livy_statement_state":"available","parent_msg_id":"243024ba-8d0d-4efb-8f15-31bc64f6927d","queued_time":"2023-09-11T18:46:17.579895Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:12.846GMT","dataRead":4458,"dataWritten":0,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"13","jobId":88,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[162,163,164],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:12.823GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:12.806GMT","dataRead":3929,"dataWritten":4458,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"13","jobId":87,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[161,160],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:12.439GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:12.340GMT","dataRead":4424,"dataWritten":3929,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"13","jobId":86,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[159],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:12.293GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:11.918GMT","dataRead":2150,"dataWritten":0,"description":"Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"13","jobId":85,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[157,158],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:11.824GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:11.754GMT","dataRead":2219,"dataWritten":6704,"description":"Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"13","jobId":84,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":56,"stageIds":[155,156],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:11.467GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:11.426GMT","dataRead":3542,"dataWritten":2219,"description":"Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"13","jobId":83,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":56,"stageIds":[154],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:11.367GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:11.147GMT","dataRead":4453,"dataWritten":0,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"13","jobId":82,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[153,151,152],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:11.117GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:11.100GMT","dataRead":1939,"dataWritten":4453,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"13","jobId":81,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[150,149],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:10.679GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:10.578GMT","dataRead":2643,"dataWritten":1939,"description":"Delta: Job group for statement 13:\ntable_name = 'dimension_supplier'\ndimension_city_schema = StructType([\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('SupplierID', IntegerType(), True), \n    StructField('Supplier', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('PrimaryContact', StringType(), True),\n    StructField('SupplierReference', StringType(), True),\n    StructField('PaymentDays', IntegerType(), True),\n    StructField('PostalCode', IntegerType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"13","jobId":80,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[148],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:10.524GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":13},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 13, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_supplier'\n","dimension_city_schema = StructType([\n","    StructField('SupplierKey', IntegerType(), True),\n","    StructField('SupplierID', IntegerType(), True), \n","    StructField('Supplier', StringType(), True),\n","    StructField('Category', StringType(), True),\n","    StructField('PrimaryContact', StringType(), True),\n","    StructField('SupplierReference', StringType(), True),\n","    StructField('PaymentDays', IntegerType(), True),\n","    StructField('PostalCode', IntegerType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTO', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"ff28d396-42d9-4b31-9c9f-2431a9a37179","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Product"]},{"cell_type":"code","execution_count":12,"id":"5f733a0a-8f8f-4861-abff-53f578e1d840","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:18.8747902Z","execution_start_time":"2023-09-11T18:47:15.2632379Z","livy_statement_state":"available","parent_msg_id":"ec81ba25-950d-4fe2-a77a-26ddb1d1a7ce","queued_time":"2023-09-11T18:46:18.0554269Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:18.211GMT","dataRead":4435,"dataWritten":0,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"14","jobId":97,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[179,180,181],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:18.181GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:18.165GMT","dataRead":5878,"dataWritten":4435,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"14","jobId":96,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[177,178],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:17.785GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:17.695GMT","dataRead":5798,"dataWritten":5878,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"14","jobId":95,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[176],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:17.608GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:17.193GMT","dataRead":2124,"dataWritten":0,"description":"Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":94,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":3,"stageIds":[174,175],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:17.089GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:17.009GMT","dataRead":7489,"dataWritten":8314,"description":"Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":93,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":156,"stageIds":[172,173],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:16.750GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:16.704GMT","dataRead":9021,"dataWritten":7489,"description":"Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"14","jobId":92,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":156,"stageIds":[171],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:16.655GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:16.437GMT","dataRead":4430,"dataWritten":0,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"14","jobId":91,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[168,169,170],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:16.414GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:16.398GMT","dataRead":3887,"dataWritten":4430,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"14","jobId":90,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[166,167],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:15.970GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:15.851GMT","dataRead":4100,"dataWritten":3887,"description":"Delta: Job group for statement 14:\ntable_name = 'dimension_product'\ndimension_product_schema = StructType([\n    StructField('Products_ID', IntegerType(), True), \n    StructField('ProductID', StringType(), True), \n    StructField('Name', StringType(), True),\n    StructField('Department', StringType(), True),\n    StructField('Category', StringType(), True),\n    StructField('SubCampaigns', StringType(), True),\n    StructField('TargetGender', StringType(), True),\n    StructField('TargetClassification', StringType(), True),\n    StructField('TargetGeneration', StringType(), True),\n    ] \n)\ndf = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"14","jobId":89,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[165],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:15.804GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":14},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 14, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_product'\n","dimension_product_schema = StructType([\n","    StructField('Products_ID', IntegerType(), True), \n","    StructField('ProductID', StringType(), True), \n","    StructField('Name', StringType(), True),\n","    StructField('Department', StringType(), True),\n","    StructField('Category', StringType(), True),\n","    StructField('SubCampaigns', StringType(), True),\n","    StructField('TargetGender', StringType(), True),\n","    StructField('TargetClassification', StringType(), True),\n","    StructField('TargetGeneration', StringType(), True),\n","    ] \n",")\n","df = spark.read.format(\"csv\").schema(dimension_product_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"31a9d26d-f104-40d5-8f38-e7099be15ecd","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Date"]},{"cell_type":"code","execution_count":13,"id":"e4653cc3-59d5-4641-9e76-98884e7ae2de","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"15\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:24.1930123Z","execution_start_time":"2023-09-11T18:47:19.2472721Z","livy_statement_state":"available","parent_msg_id":"f199a56d-41ec-4e69-889c-756e6b925943","queued_time":"2023-09-11T18:46:18.7038189Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:22.393GMT","dataRead":4425,"dataWritten":0,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":106,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[197,198,196],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:22.371GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:22.358GMT","dataRead":3751,"dataWritten":4425,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":105,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[194,195],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:21.944GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:21.805GMT","dataRead":4745,"dataWritten":3751,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"15","jobId":104,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[193],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:21.752GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:21.362GMT","dataRead":2100,"dataWritten":0,"description":"Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"15","jobId":103,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[191,192],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:21.251GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:21.184GMT","dataRead":79740,"dataWritten":37003,"description":"Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"15","jobId":102,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":6574,"stageIds":[190,189],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:20.871GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:20.836GMT","dataRead":919441,"dataWritten":79740,"description":"Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"15","jobId":101,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":6574,"stageIds":[188],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:20.640GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:20.375GMT","dataRead":4420,"dataWritten":0,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"15","jobId":100,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[186,187,185],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:20.351GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:20.334GMT","dataRead":1867,"dataWritten":4420,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"15","jobId":99,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[183,184],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:19.955GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:19.839GMT","dataRead":2956,"dataWritten":1867,"description":"Delta: Job group for statement 15:\ntable_name = 'dim_date'\ndimension_city_schema = StructType([\n    StructField('Date', StringType(), True),\n    StructField('DayNumber', IntegerType(), True), \n    StructField('Day', IntegerType(), True),\n    StructField('MonthName', StringType(), True),\n    StructField('ShortMonthName', StringType(), True),\n    StructField('CYMonthNumber', IntegerType(), True),\n    StructField('CYMonthLabel', StringType(), True),\n    StructField('CYYear', IntegerType(), True),\n    StructField('CYYearLabel', StringType(), True),\n    StructField('FYMonthNumber', IntegerType(), True),\n    StructField('FYMonthLabel', StringType(), True),\n    StructField('FYYear', IntegerType(), True),\n    StructField('FYYearLabel', StringType(), True),\n    StructField('WeekNumber', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"15","jobId":98,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[182],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:19.789GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":15},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 15, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dim_date'\n","dimension_city_schema = StructType([\n","    StructField('Date', StringType(), True),\n","    StructField('DayNumber', IntegerType(), True), \n","    StructField('Day', IntegerType(), True),\n","    StructField('MonthName', StringType(), True),\n","    StructField('ShortMonthName', StringType(), True),\n","    StructField('CYMonthNumber', IntegerType(), True),\n","    StructField('CYMonthLabel', StringType(), True),\n","    StructField('CYYear', IntegerType(), True),\n","    StructField('CYYearLabel', StringType(), True),\n","    StructField('FYMonthNumber', IntegerType(), True),\n","    StructField('FYMonthLabel', StringType(), True),\n","    StructField('FYYear', IntegerType(), True),\n","    StructField('FYYearLabel', StringType(), True),\n","    StructField('WeekNumber', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"b202a94b-6cce-4d48-a47f-42e826cb5059","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-Stockitem"]},{"cell_type":"code","execution_count":14,"id":"03a3c848-faec-4f92-bff7-0bd79ebf8361","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"16\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:29.5116013Z","execution_start_time":"2023-09-11T18:47:24.6335064Z","livy_statement_state":"available","parent_msg_id":"ca4ba5fd-f0e8-47cd-a8aa-f02a34727b4a","queued_time":"2023-09-11T18:46:19.1924945Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:27.785GMT","dataRead":4573,"dataWritten":0,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"16","jobId":115,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[215,213,214],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:27.762GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:27.746GMT","dataRead":4543,"dataWritten":4573,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"16","jobId":114,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[212,211],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:27.309GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:27.178GMT","dataRead":6104,"dataWritten":4543,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"16","jobId":113,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[210],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:27.123GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:26.735GMT","dataRead":2511,"dataWritten":0,"description":"Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"16","jobId":112,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[208,209],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:26.633GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:26.571GMT","dataRead":23461,"dataWritten":20932,"description":"Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"16","jobId":111,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":1344,"stageIds":[206,207],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:26.262GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:26.220GMT","dataRead":104282,"dataWritten":23461,"description":"Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"16","jobId":110,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":1344,"stageIds":[205],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:26.073GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:25.795GMT","dataRead":4568,"dataWritten":0,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"16","jobId":109,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[204,202,203],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:25.769GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:25.756GMT","dataRead":2303,"dataWritten":4568,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"16","jobId":108,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[201,200],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:25.327GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:25.221GMT","dataRead":3812,"dataWritten":2303,"description":"Delta: Job group for statement 16:\ntable_name = 'dimension_stockitem'\ndimension_city_schema = StructType([\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('StockItemID', IntegerType(), True), \n    StructField('StockItem', StringType(), True),\n    StructField('Color', StringType(), True),\n    StructField('SellingPackage', StringType(), True),\n    StructField('BuyingPackage', StringType(), True),\n    StructField('Brand', StringType(), True),\n    StructField('Size', StringType(), True),\n    StructField('LeadTimeDay', IntegerType(), True),\n    StructField('QuantityPerOuter', IntegerType(), True),\n    StructField('IsChillerStock', StringType(), True),\n    StructField('Barcode', StringType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('RecommendedRetailPrice', DecimalType(), True),\n    StructField('WeightPerUnit', DecimalType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"16","jobId":107,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[199],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:25.177GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":16},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 16, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_stockitem'\n","dimension_city_schema = StructType([\n","    StructField('StockItemKey', IntegerType(), True),\n","    StructField('StockItemID', IntegerType(), True), \n","    StructField('StockItem', StringType(), True),\n","    StructField('Color', StringType(), True),\n","    StructField('SellingPackage', StringType(), True),\n","    StructField('BuyingPackage', StringType(), True),\n","    StructField('Brand', StringType(), True),\n","    StructField('Size', StringType(), True),\n","    StructField('LeadTimeDay', IntegerType(), True),\n","    StructField('QuantityPerOuter', IntegerType(), True),\n","    StructField('IsChillerStock', StringType(), True),\n","    StructField('Barcode', StringType(), True),\n","    StructField('TaxRate', IntegerType(), True),\n","    StructField('UnitPrice', IntegerType(), True),\n","    StructField('RecommendedRetailPrice', DecimalType(), True),\n","    StructField('WeightPerUnit', DecimalType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTO', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"20f748f1-adb6-4cad-81d4-0dfdd81b877b","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Dimension-TransactionType"]},{"cell_type":"code","execution_count":15,"id":"213fb8b4-cd56-442e-9fe1-2b164c5958d7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:33.6366461Z","execution_start_time":"2023-09-11T18:47:29.9647977Z","livy_statement_state":"available","parent_msg_id":"88752bfe-634c-4c60-859b-9ea9a087451e","queued_time":"2023-09-11T18:46:19.6507143Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:32.697GMT","dataRead":4374,"dataWritten":0,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"17","jobId":124,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[230,231,232],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:32.675GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:32.658GMT","dataRead":3436,"dataWritten":4374,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"17","jobId":123,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[228,229],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:32.227GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:32.140GMT","dataRead":3369,"dataWritten":3436,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"17","jobId":122,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[227],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:32.097GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:31.719GMT","dataRead":1872,"dataWritten":0,"description":"Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"17","jobId":121,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[225,226],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:31.602GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:31.532GMT","dataRead":642,"dataWritten":3615,"description":"Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"17","jobId":120,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":30,"stageIds":[223,224],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:31.289GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:31.254GMT","dataRead":1128,"dataWritten":642,"description":"Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"17","jobId":119,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":30,"stageIds":[222],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:31.218GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:30.967GMT","dataRead":4369,"dataWritten":0,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"17","jobId":118,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[219,220,221],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:30.939GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:30.921GMT","dataRead":1645,"dataWritten":4369,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"17","jobId":117,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[217,218],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:30.600GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:30.477GMT","dataRead":1920,"dataWritten":1645,"description":"Delta: Job group for statement 17:\ntable_name = 'dimension_transactiontype'\ndimension_city_schema = StructType([\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('TransactionTypeID', IntegerType(), True), \n    StructField('TransactionType', StringType(), True),\n    StructField('ValidFrom', StringType(), True),\n    StructField('ValidTO', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"17","jobId":116,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[216],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:30.431GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":17},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 17, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'dimension_transactiontype'\n","dimension_city_schema = StructType([\n","    StructField('TransactionTypeKey', IntegerType(), True),\n","    StructField('TransactionTypeID', IntegerType(), True), \n","    StructField('TransactionType', StringType(), True),\n","    StructField('ValidFrom', StringType(), True),\n","    StructField('ValidTO', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Dim+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"96dddfc9-5b3c-411d-a4c0-4659899bde97","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Movement"]},{"cell_type":"code","execution_count":16,"id":"01b27ccb-cde7-43c0-809a-6c2da79c7a63","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:38.9940608Z","execution_start_time":"2023-09-11T18:47:34.0438783Z","livy_statement_state":"available","parent_msg_id":"a4ad1507-efb0-47a1-9867-ee714bd050dc","queued_time":"2023-09-11T18:46:20.0927675Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:37.779GMT","dataRead":4455,"dataWritten":0,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"18","jobId":133,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[248,249,247],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:37.737GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:37.667GMT","dataRead":3727,"dataWritten":4455,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"18","jobId":132,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[245,246],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:37.213GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:37.109GMT","dataRead":4317,"dataWritten":3727,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"18","jobId":131,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[244],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:37.064GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:36.659GMT","dataRead":2066,"dataWritten":0,"description":"Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"18","jobId":130,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[242,243],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:36.565GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:36.490GMT","dataRead":7105164,"dataWritten":3275587,"description":"Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"18","jobId":129,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":4,"numTasks":5,"rowCount":473334,"stageIds":[241,240],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:35.758GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:35.718GMT","dataRead":14445879,"dataWritten":7105164,"description":"Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"18","jobId":128,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":4,"numCompletedStages":1,"numCompletedTasks":4,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":4,"rowCount":473334,"stageIds":[239],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:35.349GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:35.136GMT","dataRead":4450,"dataWritten":0,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"18","jobId":127,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[237,238,236],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:35.114GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:35.098GMT","dataRead":1838,"dataWritten":4450,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"18","jobId":126,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[234,235],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:34.690GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:34.590GMT","dataRead":2600,"dataWritten":1838,"description":"Delta: Job group for statement 18:\ntable_name = 'fact_movement'\ndimension_city_schema = StructType([\n    StructField('MovementKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('CustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypeKey', IntegerType(), True),\n    StructField('StockItemTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"18","jobId":125,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[233],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:34.547GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":18},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 18, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_movement'\n","dimension_city_schema = StructType([\n","    StructField('MovementKey', IntegerType(), True),\n","    StructField('DateKey', StringType(), True), \n","    StructField('StockItemKey', IntegerType(), True),\n","    StructField('CustomerKey', IntegerType(), True),\n","    StructField('SupplierKey', IntegerType(), True),\n","    StructField('TransactionTypeKey', IntegerType(), True),\n","    StructField('StockItemTransactionID', IntegerType(), True),\n","    StructField('InvoiceID', IntegerType(), True),\n","    StructField('PurchaseOrderID', IntegerType(), True),\n","    StructField('Quantity', IntegerType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"078dce73-e174-4be7-b87b-72a745047982","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Order"]},{"cell_type":"code","execution_count":17,"id":"0340fb62-a8f5-4176-8b5f-f12d75747edd","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"19\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:45.8985232Z","execution_start_time":"2023-09-11T18:47:39.4070683Z","livy_statement_state":"available","parent_msg_id":"5c3bf667-3b1f-4c93-8204-738af761693e","queued_time":"2023-09-11T18:46:20.425031Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:43.631GMT","dataRead":4567,"dataWritten":0,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"19","jobId":142,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[266,264,265],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:43.608GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:43.593GMT","dataRead":4408,"dataWritten":4567,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"19","jobId":141,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[263,262],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:43.227GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:43.143GMT","dataRead":6023,"dataWritten":4408,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"19","jobId":140,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[261],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:43.098GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:42.729GMT","dataRead":2458,"dataWritten":0,"description":"Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"19","jobId":139,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[259,260],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:42.638GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:42.571GMT","dataRead":11525912,"dataWritten":3543020,"description":"Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"19","jobId":138,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":462824,"stageIds":[257,258],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:41.490GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:41.448GMT","dataRead":35726826,"dataWritten":11525912,"description":"Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"19","jobId":137,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":462824,"stageIds":[256],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:40.739GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:40.517GMT","dataRead":4562,"dataWritten":0,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"19","jobId":136,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[255,253,254],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:40.495GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:40.478GMT","dataRead":2236,"dataWritten":4562,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"19","jobId":135,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[251,252],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:40.109GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:39.990GMT","dataRead":3774,"dataWritten":2236,"description":"Delta: Job group for statement 19:\ntable_name = 'fact_order'\ndimension_city_schema = StructType([\n    StructField('OrderKey', IntegerType(), True),\n    StructField('CityKey', IntegerType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('OrderDateKey', StringType(), True),\n    StructField('PickedDatekey', StringType(), True),\n    StructField('SalesPersonKey', IntegerType(), True),\n    StructField('PickerKey', IntegerType(), True),\n    StructField('OrderID', IntegerType(), True),\n    StructField('BackOrderID', IntegerType(), True),\n    StructField('Description', StringType(), True),\n    StructField('Package', StringType(), True),\n    StructField('Quantity', IntegerType(), True),\n    StructField('UnitPrice', IntegerType(), True),\n    StructField('TaxRate', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', DecimalType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('Li...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"19","jobId":134,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[250],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:39.947GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":19},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 19, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_order'\n","dimension_city_schema = StructType([\n","    StructField('OrderKey', IntegerType(), True),\n","    StructField('CityKey', IntegerType(), True), \n","    StructField('CustomerKey', IntegerType(), True),\n","    StructField('StockItemKey', IntegerType(), True),\n","    StructField('OrderDateKey', StringType(), True),\n","    StructField('PickedDatekey', StringType(), True),\n","    StructField('SalesPersonKey', IntegerType(), True),\n","    StructField('PickerKey', IntegerType(), True),\n","    StructField('OrderID', IntegerType(), True),\n","    StructField('BackOrderID', IntegerType(), True),\n","    StructField('Description', StringType(), True),\n","    StructField('Package', StringType(), True),\n","    StructField('Quantity', IntegerType(), True),\n","    StructField('UnitPrice', IntegerType(), True),\n","    StructField('TaxRate', IntegerType(), True),\n","    StructField('TotalExcludingTax', IntegerType(), True),\n","    StructField('TaxAmount', DecimalType(), True),\n","    StructField('TotalIncludingTax', DecimalType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"d4bba9a0-a265-4b9f-850d-e44566080aed","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Purchase"]},{"cell_type":"code","execution_count":18,"id":"f058b472-63c6-4eda-81a4-ffb544f084bc","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:49.9977206Z","execution_start_time":"2023-09-11T18:47:46.358715Z","livy_statement_state":"available","parent_msg_id":"3714c26f-ec9f-4554-b2b6-875f2995f642","queued_time":"2023-09-11T18:46:20.8820769Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:49.061GMT","dataRead":4454,"dataWritten":0,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"20","jobId":151,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[281,282,283],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:49.037GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:49.016GMT","dataRead":3780,"dataWritten":4454,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"20","jobId":150,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[279,280],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:48.681GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:48.585GMT","dataRead":4313,"dataWritten":3780,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"20","jobId":149,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[278],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:48.540GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:48.149GMT","dataRead":2088,"dataWritten":0,"description":"Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"20","jobId":148,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[277,276],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:48.047GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:47.969GMT","dataRead":239642,"dataWritten":169481,"description":"Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"20","jobId":147,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":16734,"stageIds":[274,275],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:47.686GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:47.649GMT","dataRead":563447,"dataWritten":239642,"description":"Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"20","jobId":146,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":16734,"stageIds":[273],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:47.549GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:47.335GMT","dataRead":4449,"dataWritten":0,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"20","jobId":145,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[270,271,272],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:47.308GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:47.280GMT","dataRead":1866,"dataWritten":4449,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"20","jobId":144,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[269,268],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:46.949GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:46.854GMT","dataRead":2596,"dataWritten":1866,"description":"Delta: Job group for statement 20:\ntable_name = 'fact_purchase'\ndimension_city_schema = StructType([\n    StructField('PurchaseKey', IntegerType(), True), \n    StructField('DateKey', StringType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('StockItemKey', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('OrderedOuters', IntegerType(), True),\n    StructField('OrderedQuantity', IntegerType(), True),\n    StructField('ReceivedOuters', IntegerType(), True),\n  StructField('Package', StringType(), True),\n    StructField('IsOrderFinalized', StringType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"20","jobId":143,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[267],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:46.810GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":20},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 20, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_purchase'\n","dimension_city_schema = StructType([\n","    StructField('PurchaseKey', IntegerType(), True), \n","    StructField('DateKey', StringType(), True),\n","    StructField('SupplierKey', IntegerType(), True),\n","    StructField('StockItemKey', IntegerType(), True),\n","    StructField('PurchaseOrderID', IntegerType(), True),\n","    StructField('OrderedOuters', IntegerType(), True),\n","    StructField('OrderedQuantity', IntegerType(), True),\n","    StructField('ReceivedOuters', IntegerType(), True),\n","  StructField('Package', StringType(), True),\n","    StructField('IsOrderFinalized', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"5abca51e-80a3-48eb-bc8d-e079f33a8982","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Transaction"]},{"cell_type":"code","execution_count":19,"id":"0e739a97-4659-47e8-b60b-460d777878f3","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b\",\"applicationId\":\"application_1694457631810_0001\",\"jobGroupId\":\"21\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:55.3698328Z","execution_start_time":"2023-09-11T18:47:50.4035492Z","livy_statement_state":"available","parent_msg_id":"7baa1332-5d37-4bb3-998b-9d2992c7e2f1","queued_time":"2023-09-11T18:46:21.2230752Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:54.126GMT","dataRead":4582,"dataWritten":0,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"21","jobId":160,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[299,300,298],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:54.104GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:54.089GMT","dataRead":4315,"dataWritten":4582,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"21","jobId":159,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[296,297],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:53.721GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:53.618GMT","dataRead":6134,"dataWritten":4315,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"21","jobId":158,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[295],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:53.571GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:53.189GMT","dataRead":2423,"dataWritten":0,"description":"Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"21","jobId":157,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":3,"stageIds":[293,294],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:53.101GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:53.038GMT","dataRead":2831045,"dataWritten":2191227,"description":"Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"21","jobId":156,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":3,"rowCount":199170,"stageIds":[291,292],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:52.154GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:52.119GMT","dataRead":8166221,"dataWritten":2831045,"description":"Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"21","jobId":155,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":199170,"stageIds":[290],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:51.619GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:51.410GMT","dataRead":4577,"dataWritten":0,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"21","jobId":154,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[287,288,289],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:51.388GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:51.373GMT","dataRead":2193,"dataWritten":4577,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"21","jobId":153,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":54,"stageIds":[285,286],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:51.042GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:50.943GMT","dataRead":3824,"dataWritten":2193,"description":"Delta: Job group for statement 21:\ntable_name = 'fact_transaction'\ndimension_city_schema = StructType([\n    StructField('TransactionKey', IntegerType(), True),\n    StructField('DateKey', StringType(), True), \n    StructField('CustomerKey', IntegerType(), True),\n    StructField('BillToCustomerKey', IntegerType(), True),\n    StructField('SupplierKey', IntegerType(), True),\n    StructField('TransactionTypekey', IntegerType(), True),\n    StructField('PaymentMethodKey', IntegerType(), True),\n    StructField('CustomerTransactionID', IntegerType(), True),\n    StructField('SupplierTransactionID', IntegerType(), True),\n    StructField('InvoiceID', IntegerType(), True),\n    StructField('PurchaseOrderID', IntegerType(), True),\n    StructField('SupplierInvoiceNumber', IntegerType(), True),\n    StructField('TotalExcludingTax', IntegerType(), True),\n    StructField('TaxAmount', IntegerType(), True),\n    StructField('TotalIncludingTax', DecimalType(), True),\n    StructField('OutstandingBalance', IntegerType(), True),\n    StructField('IsFinalized', StringType...: Compute snapshot for version: 0","displayName":"toString at String.java:2994","jobGroup":"21","jobId":152,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":8,"stageIds":[284],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:50.890GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":21},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 21, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_transaction'\n","dimension_city_schema = StructType([\n","    StructField('TransactionKey', IntegerType(), True),\n","    StructField('DateKey', StringType(), True), \n","    StructField('CustomerKey', IntegerType(), True),\n","    StructField('BillToCustomerKey', IntegerType(), True),\n","    StructField('SupplierKey', IntegerType(), True),\n","    StructField('TransactionTypekey', IntegerType(), True),\n","    StructField('PaymentMethodKey', IntegerType(), True),\n","    StructField('CustomerTransactionID', IntegerType(), True),\n","    StructField('SupplierTransactionID', IntegerType(), True),\n","    StructField('InvoiceID', IntegerType(), True),\n","    StructField('PurchaseOrderID', IntegerType(), True),\n","    StructField('SupplierInvoiceNumber', IntegerType(), True),\n","    StructField('TotalExcludingTax', IntegerType(), True),\n","    StructField('TaxAmount', IntegerType(), True),\n","    StructField('TotalIncludingTax', DecimalType(), True),\n","    StructField('OutstandingBalance', IntegerType(), True),\n","    StructField('IsFinalized', StringType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"17f5896a-158a-4b95-aa67-acbed50d323a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Stockholding"]},{"cell_type":"code","execution_count":20,"id":"916c03ab-8d76-406d-b345-463fa9ee97c7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T18:47:59.39825Z","execution_start_time":"2023-09-11T18:47:55.7519814Z","livy_statement_state":"available","parent_msg_id":"e70aa17a-d25c-4527-99d1-945b16eb6930","queued_time":"2023-09-11T18:46:21.657855Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T18:47:58.484GMT","dataRead":4448,"dataWritten":0,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"22","jobId":169,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":53,"numTasks":54,"rowCount":50,"stageIds":[317,315,316],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:58.462GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:58.449GMT","dataRead":5128,"dataWritten":4448,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"22","jobId":168,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":3,"numTasks":53,"rowCount":60,"stageIds":[314,313],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:58.060GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:57.970GMT","dataRead":5423,"dataWritten":5128,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 2","displayName":"toString at String.java:2994","jobGroup":"22","jobId":167,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":3,"numCompletedStages":1,"numCompletedTasks":3,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":3,"rowCount":20,"stageIds":[312],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:57.922GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:57.580GMT","dataRead":2009,"dataWritten":0,"description":"Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"22","jobId":166,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":3,"stageIds":[310,311],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:57.496GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:57.425GMT","dataRead":6423,"dataWritten":9686,"description":"Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"22","jobId":165,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":2,"rowCount":454,"stageIds":[308,309],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:57.193GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:57.159GMT","dataRead":8456,"dataWritten":6423,"description":"Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"22","jobId":164,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":454,"stageIds":[307],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:57.104GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:56.905GMT","dataRead":4443,"dataWritten":0,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"22","jobId":163,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":52,"numTasks":53,"rowCount":50,"stageIds":[305,306,304],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:56.878GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:56.864GMT","dataRead":3270,"dataWritten":4443,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"22","jobId":162,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":2,"numTasks":52,"rowCount":57,"stageIds":[302,303],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:56.426GMT","usageDescription":""},{"completionTime":"2023-09-11T18:47:56.300GMT","dataRead":3838,"dataWritten":3270,"description":"Delta: Job group for statement 22:\ntable_name = 'fact_stockholding'\ndimension_city_schema = StructType([\n    StructField('StockHoldingKey', IntegerType(), True), \n    StructField('StockItemKey', IntegerType(), True),\n    StructField('QuantityOnHand', IntegerType(), True),\n    StructField('BinLocation', StringType(), True),\n    StructField('LastStocktakeQuantity', IntegerType(), True),\n    StructField('LastCostPrice', IntegerType(), True),\n    StructField('ReorderLevel', IntegerType(), True),\n    StructField('TargetStockLevel', IntegerType(), True),\n    StructField('LineageKey', IntegerType(), True)])\n\ndf = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 1","displayName":"toString at String.java:2994","jobGroup":"22","jobId":161,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":2,"numCompletedStages":1,"numCompletedTasks":2,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":2,"rowCount":14,"stageIds":[301],"status":"SUCCEEDED","submissionTime":"2023-09-11T18:47:56.249GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":9,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":22},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 22, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_stockholding'\n","dimension_city_schema = StructType([\n","    StructField('StockHoldingKey', IntegerType(), True), \n","    StructField('StockItemKey', IntegerType(), True),\n","    StructField('QuantityOnHand', IntegerType(), True),\n","    StructField('BinLocation', StringType(), True),\n","    StructField('LastStocktakeQuantity', IntegerType(), True),\n","    StructField('LastCostPrice', IntegerType(), True),\n","    StructField('ReorderLevel', IntegerType(), True),\n","    StructField('TargetStockLevel', IntegerType(), True),\n","    StructField('LineageKey', IntegerType(), True)])\n","\n","df = spark.read.format(\"csv\").schema(dimension_city_schema).option(\"header\",\"true\").load(Path_Fact+table_name)\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"22c7273d-cd07-4f28-bdba-cd775223557f","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Campaigndata"]},{"cell_type":"code","execution_count":24,"id":"0c8c5e93-6b88-4f03-bcb9-626037cfad8d","metadata":{"advisor":{"adviceMetadata":"{\"artifactId\":\"7eb7090e-e87e-4f18-86b5-44a0a1d259ed\",\"activityId\":\"791d9c65-d23e-4b4f-9165-1d4b1e928963\",\"applicationId\":\"application_1694465880208_0001\",\"jobGroupId\":\"5\",\"advices\":{\"info\":1}}"},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"parent_msg_id":"43f0f552-24cb-40c8-b645-e35aca24d19b","queued_time":"2023-09-11T20:59:27.0113322Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["table_name = 'fact_campaigndata'\n","\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_Fact+table_name)\n","df = df.select(col(\"Region\"),col(\"Country\"),col(\"ProductCategory\"),col(\"Campaign_ID\"),col(\"Campaign_Name\"),\n","col(\"Qualification\"),col(\"Qualification_Number\"),col(\"Response_Status\"),col(\"Responses\"),col(\"Cost\"),col(\"Revenue\"),col(\"ROI\"),col(\"Lead_Generation\"),col(\"Revenue_Target\"),col(\"Customer_Segment\"),\n","col(\"Profit\"),col(\"Marketing_Cost\"),col(\"CampaignID\"))\n","\n","df = df.withColumn(\"ProductCategory\",col(\"ProductCategory\").cast(\"string\"))\n","df = df.withColumn(\"CampaignID\",col(\"CampaignID\").cast(\"integer\")) \n","df = df.withColumn(\"Campaign_ID\",col(\"Campaign_ID\").cast(\"integer\"))\n","df = df.withColumn(\"ROI\",col(\"ROI\").cast(\"integer\")) \n","df = df.withColumn(\"Revenue_Target\",col(\"Revenue_Target\").cast(\"Double\"))\n","df = df.withColumn(\"Cost\",col(\"Cost\").cast(\"Double\"))\n","df = df.withColumn(\"Responses\",col(\"Responses\").cast(\"Double\"))\n","df = df.withColumn(\"Revenue\",col(\"Revenue\").cast(\"Double\"))\n","df = df.withColumn(\"Profit\",col(\"Profit\").cast(\"Double\"))\n","df = df.withColumn(\"Marketing_Cost\",col(\"Marketing_Cost\").cast(\"Double\"))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"bc84709e-ac4c-42c5-abf9-5052f7d5f208","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-Sales"]},{"cell_type":"code","execution_count":35,"id":"805b1f7f-c70c-4f7c-ab05-4b7638ffd4f3","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2023-09-11T19:06:41.0859598Z","execution_start_time":"2023-09-11T19:06:30.9508492Z","livy_statement_state":"available","parent_msg_id":"c13e4c92-e2a1-4b16-99f4-4ac4872a6775","queued_time":"2023-09-11T19:06:30.5417676Z","session_id":"4259ac00-d937-4ed0-b39b-3a46ab7c1f3b","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2023-09-11T19:06:38.865GMT","dataRead":4663,"dataWritten":0,"description":"Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"37","jobId":195,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":56,"numTasks":57,"rowCount":50,"stageIds":[357,358,359],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:38.833GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:38.812GMT","dataRead":428198,"dataWritten":4663,"description":"Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"37","jobId":194,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":6,"numTasks":56,"rowCount":3891,"stageIds":[356,355],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:38.231GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:38.115GMT","dataRead":1883321,"dataWritten":428198,"description":"Delta: Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name): Compute snapshot for version: 5","displayName":"toString at String.java:2994","jobGroup":"37","jobId":193,"killedTasksSummary":{},"name":"toString at String.java:2994","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":6,"numCompletedStages":1,"numCompletedTasks":6,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":6,"rowCount":7682,"stageIds":[354],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:37.969GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:37.513GMT","dataRead":186764,"dataWritten":0,"description":"Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"37","jobId":192,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":5,"numTasks":55,"rowCount":1918,"stageIds":[353,352],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:37.398GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:37.337GMT","dataRead":50088514,"dataWritten":12832141,"description":"Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"37","jobId":191,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":8,"numTasks":9,"rowCount":4000006,"stageIds":[350,351],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:35.344GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:35.309GMT","dataRead":170830001,"dataWritten":50088514,"description":"Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","jobGroup":"37","jobId":190,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperationInternal$1 at SynapseLoggingShim.scala:95","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":8,"numCompletedStages":1,"numCompletedTasks":8,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":8,"rowCount":4000006,"stageIds":[349],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:32.411GMT","usageDescription":""},{"completionTime":"2023-09-11T19:06:31.707GMT","dataRead":65536,"dataWritten":0,"description":"Job group for statement 37:\ntable_name='fact_sales'\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\ndf = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n\ndf = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\ndf = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\ndf = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\ndf = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\ndf = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\ndf = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\ndf = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\ndf = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"lakehouseSilver.\"+table_name)","displayName":"load at <unknown>:0","jobGroup":"37","jobId":189,"killedTasksSummary":{},"name":"load at <unknown>:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":1,"stageIds":[348],"status":"SUCCEEDED","submissionTime":"2023-09-11T19:06:31.558GMT","usageDescription":""}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":7,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":37},"text/plain":["StatementMeta(, 4259ac00-d937-4ed0-b39b-3a46ab7c1f3b, 37, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["table_name='fact_sales'\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_LitwareData+'/*/*/*.txt')\n","df = df.select(col(\"CustomerId\"),col(\"ProductId\"),col(\"Quantity\"),col(\"Price\"),col(\"TotalAmount\"),col(\"TransactionDate\"),col(\"ProfitAmount\"),col(\"StoreId\"))\n","\n","df = df.withColumn(\"ProductId\",col(\"ProductId\").cast(\"integer\"))\n","df = df.withColumn(\"CustomerId\",col(\"CustomerId\").cast(\"integer\"))\n","df = df.withColumn(\"Quantity\",col(\"Quantity\").cast(\"integer\"))\n","df = df.withColumn(\"Price\",col(\"Price\").cast(\"integer\"))\n","df = df.withColumn(\"TotalAmount\",col(\"TotalAmount\").cast(\"Double\"))\n","df = df.withColumn(\"ProfitAmount\",col(\"ProfitAmount\").cast(\"Double\"))\n","df = df.withColumn(\"StoreId\",col(\"StoreId\").cast(\"integer\"))\n","df = df.withColumn(\"TransactionDate\",to_date(unix_timestamp(col('TransactionDate'), 'MM-dd-yyyy').cast(\"timestamp\")))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]},{"cell_type":"markdown","id":"6eaffadb-872a-4976-b2e4-ced0d598080e","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Fact-StoreSalesData"]},{"cell_type":"code","execution_count":42,"id":"8cf65a10-1457-4c2f-a580-c123f0fc7539","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"parent_msg_id":"8ad427e8-1fc3-4b21-ac3d-a57ee4815449","queued_time":"2023-09-11T22:37:33.1000049Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["table_name='fact_store_sales_data'\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(Path_Fact+table_name)\n","df=df.withColumn(\"store\",col(\"store\").cast(\"integer\"))\n","df=df.withColumn(\"item\",col(\"item\").cast(\"integer\"))\n","df=df.withColumn(\"sales\",col(\"sales\").cast(\"integer\"))\n","\n","df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"#LAKEHOUSE_SILVER#.\"+table_name)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"host":{"synapse_widget":{"state":{},"token":"ade1524a-8ed7-4bad-8cd1-4ff6a91fe735"}},"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"65cb467b-e047-4716-9d4b-6bc2d1394a72","default_lakehouse_name":"lakehouseSilver","default_lakehouse_workspace_id":"a5271330-478b-4f25-b793-db6ec1971842","known_lakehouses":[{"id":"65cb467b-e047-4716-9d4b-6bc2d1394a72"}]}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
