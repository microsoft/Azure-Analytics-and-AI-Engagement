{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03519625-c336-49d3-9498-f7e62cee40fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install required external libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d8ccdfe-c0cf-44e3-9599-334aa8f292fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.30.2 \"unstructured[pdf,docx]==0.10.30\" langchain==0.1.5 llama-index==0.9.3 databricks-vectorsearch==0.22 pydantic==1.10.9 mlflow==2.10.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32253b7c-95c3-4ad2-af31-4d26880ee084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Init our resources and catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29a9621-f046-4c60-8dce-3e5aea40389c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00-init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b690608-abb2-4818-ac96-f7bfed86d4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###pdf files are available in Volume (or DBFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cde9a987-4418-4fee-a890-37aa39ea638d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List our raw PDF docs\n",
    "volume_folder =  f\"/Volumes/litware_unity_catalog/rag/documents_store/pdf_documents\"   \n",
    "\n",
    "display(dbutils.fs.ls(volume_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a83d0c8-132b-4067-a6eb-fee5e80fe0ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transform pdf as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ce001d-0642-4529-a93e-2adfbbda7331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-30 12:33:34.350607: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-30 12:33:34.505367: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-30 12:33:35.099253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-30 12:33:40.516717: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import re\n",
    "\n",
    "def extract_doc_text(x : bytes) -> str:\n",
    "  # Read files and extract the values with unstructured\n",
    "  sections = partition(file=io.BytesIO(x))\n",
    "  def clean_section(txt):\n",
    "    txt = re.sub(r'\\n', '', txt)\n",
    "    return re.sub(r' ?\\.', '.', txt)\n",
    "  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.\n",
    "  return \"\\n\".join([clean_section(s.text) for s in sections]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ca251-c747-4e24-8677-39f706c4071d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS litware_unity_catalog.rag.documents_embedding (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding STRING\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66560e1-1b7d-4383-a5b2-bd35e8e9f14d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create the final documents_embedding table containing chunks and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd3d284-4ae1-4a72-9392-db0db8223881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+--------------------+\n",
      "|                path|   modificationTime|length|             content|                text|\n",
      "+--------------------+-------------------+------+--------------------+--------------------+\n",
      "|dbfs:/Volumes/lit...|2025-07-28 12:03:18|634144|[25 50 44 46 2D 3...|Quarterly Market ...|\n",
      "|dbfs:/Volumes/lit...|2025-07-28 12:03:18|634144|[25 50 44 46 2D 3...|In summary, Conto...|\n",
      "+--------------------+-------------------+------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "import io\n",
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "# Define the function to read data as chunks\n",
    "def read_as_chunk(text):\n",
    "    # set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    # Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    txt = extract_doc_text(text)\n",
    "    nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "    return [n.text for n in nodes]\n",
    "\n",
    "# Register the UDF\n",
    "read_as_chunk_udf = F.udf(read_as_chunk, F.ArrayType(F.StringType()))\n",
    "\n",
    "# Read data from the 'pdf_raw' table\n",
    "df = spark.read.table(f\"{catalog}.{db}.documents_raw\")\n",
    "\n",
    "# Use withColumn and the UDF to add a new column\n",
    "df = df.withColumn(\"text\", F.explode(read_as_chunk_udf(df.content)))\n",
    "\n",
    "# Show the modified DataFrame\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56f29416-4cd2-4a19-9156-99d1cdd1bf4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Computing the chunk embeddings and saving them to our Delta Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d18edbb-2816-416f-8e07-4620e9d94250",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+--------------------+--------------------+\n",
      "|                path|   modificationTime|length|             content|                text|           embedding|\n",
      "+--------------------+-------------------+------+--------------------+--------------------+--------------------+\n",
      "|dbfs:/Volumes/lit...|2025-07-28 12:03:18|634144|[25 50 44 46 2D 3...|Quarterly Market ...|[0.00300407409667...|\n",
      "|dbfs:/Volumes/lit...|2025-07-28 12:03:18|634144|[25 50 44 46 2D 3...|In summary, Conto...|[0.01162719726562...|\n",
      "+--------------------+-------------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow.deployments\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")  # Assuming MLflow is configured\n",
    "\n",
    "    try:\n",
    "        response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [text]}) #â€‚if required use: databricks-bge-large-en\n",
    "        return response.data[0]['embedding']  # Extract embedding for the single input\n",
    "    except Exception as e:\n",
    "        return None  # Handle potential errors\n",
    "\n",
    "\n",
    "get_embedding_udf = F.udf(get_embedding, F.StringType())  # Adjusted return type for single string\n",
    "df = df.withColumn(\"embedding\", get_embedding_udf(F.col(\"text\")))  # Apply UDF to text column\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625406ad-f9af-4605-a141-8fed6e6a53f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Store embedding table into Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d34786-713e-4523-87e9-a9f1a0dda4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat\n",
    "\n",
    "df.selectExpr('path as url', 'text as content', 'embedding') \\\n",
    "  .write \\\n",
    "  .mode(\"append\") \\\n",
    "  .format(\"delta\") \\\n",
    "  .saveAsTable(f\"{catalog}.{db}.documents_embedding\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7417746149314976,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Ingesting and preparing PDF for LLM and Self Managed Vector Search Embeddings (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
