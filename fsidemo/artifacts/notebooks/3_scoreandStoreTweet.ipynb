{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "marketriskspksm",
              "session_id": 73,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-11-18T21:56:10.7080148Z",
              "execution_start_time": "2020-11-18T22:00:34.4428501Z",
              "execution_finish_time": "2020-11-18T22:00:36.4793209Z"
            },
            "text/plain": "StatementMeta(marketriskspksm, 73, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "zipp 0.6.0\nzict 1.0.0\nyfinance 0.1.54\nxlwt 1.2.0\nXlsxWriter 0.9.6\nxlrd 1.0.0\nxgboost 0.90\nwrapt 1.11.2\nwidgetsnbextension 2.0.0\nwheel 0.30.0\nWerkzeug 0.16.1\nwebsocket-client 0.56.0\nwcwidth 0.2.5\nvega-datasets 0.7.0\nurllib3 1.26.2\nunicodecsv 0.14.1\ntyping-extensions 3.7.4\ntyped-ast 1.4.1\ntraitlets 4.3.3\ntqdm 4.51.0\ntornado 6.0.3\ntorch 1.4.0\ntoolz 0.10.0\ntestpath 0.3\nterminado 0.6\ntermcolor 1.1.0\ntensorflow 1.14.0\ntensorflow-estimator 1.14.0\ntensorboard 1.14.0\ntblib 1.4.0\ntables 3.3.0\nsympy 1.0\nstatsmodels 0.12.1\nSQLAlchemy 1.1.9\nspyder 3.1.4\nsortedcontainers 2.1.0\nsortedcollections 0.5.3\nsnowballstemmer 1.2.1\nsmart-open 1.8.4\nsklearn-pandas 1.7.0\nskl2onnx 1.4.9\nsix 1.15.0\nsingledispatch 3.4.0.3\nsimplegeneric 0.8.1\nshap 0.34.0\nsetuptools 50.3.2\nSecretStorage 3.1.1\nseaborn 0.11.0\nscipy 1.5.4\nscikit-learn 0.20.3\nscikit-image 0.15.0\ns3transfer 0.2.1\nruamel.yaml 0.16.10\nruamel.yaml.clib 0.2.2\nrope-py3k 0.9.4.post1\nretrying 1.3.3\nResource 0.2.1\nrequests 2.25.0\nrequests-oauthlib 1.3.0\nratelimit 2.2.1\nQtPy 1.2.1\nqtconsole 4.3.0\nQtAwesome 0.4.4\npyzmq 16.0.2\nPyYAML 5.1.2\nPyWavelets 1.0.3\npytz 2020.4\npytools 2020.4.3\nPython-EasyConfig 0.1.7\npython-dateutil 2.8.1\npytest 3.0.7\npyspark 2.4.4\npyrsistent 0.15.4\npyparsing 2.4.7\npyOpenSSL 19.0.0\npyopencl 2020.1\npyodbc 4.0.16\npymssql 2.1.4\npylint 1.6.4\nPyJWT 1.7.1\nPygments 2.7.2\npygal 2.4.0\npyflakes 1.5.0\npycurl 7.43.0\npyct 0.4.6\npycrypto 2.6.1\npycparser 2.19\npycosat 0.6.2\npyasn1 0.4.7\npyarrow 0.15.1\npy4j 0.10.7\npy 1.4.33\npy-cpuinfo 5.0.0\nptyprocess 0.6.0\npsutil 5.2.2\nprotobuf 3.10.0\nprompt-toolkit 3.0.8\nportalocker 1.7.1\npmdarima 1.1.1\nply 3.10\nplotly 4.1.1\npip 9.0.1\nPillow 8.0.1\npickleshare 0.7.5\npexpect 4.8.0\npep8 1.7.0\npatsy 0.5.1\npathspec 0.6.0\npathlib2 2.2.1\npartd 1.0.0\nparso 0.7.1\nparam 1.9.2\npandocfilters 1.4.1\npandas 1.1.4\npandas-datareader 0.9.0\npackaging 19.2\nopenpyxl 2.4.7\nonnxruntime 1.0.0\nonnxmltools 1.4.1\nonnxconverter-common 1.6.0\nonnx 1.6.0\nolefile 0.44\nodo 0.5.0\noauthlib 3.1.0\nnumpydoc 0.6.0\nnumpy 1.19.4\nnumexpr 2.6.2\nnumba 0.33.0\nnotebookutils 2.4.4-20201103.2\nnotebook 5.0.0\nnose 1.3.7\nnltk 3.2.3\nnimbusml 1.7.1\nnetworkx 2.3\nndg-httpsclient 0.5.1\nnbformat 4.3.0\nnbconvert 5.1.1\nnavigator-updater 0.1.0\nmypy 0.780\nmypy-extensions 0.4.3\nmultitasking 0.0.9\nmultipledispatch 0.4.9\nmultimethods 1.0.0\nmsrestazure 0.6.2\nmsrest 0.6.19\nmsgpack 0.6.2\nmsgpack-python 0.4.8\nmsal 1.6.0\nmsal-extensions 0.1.3\nmpmath 0.19\nmore-itertools 7.2.0\nmmlspark 1.0.0.dev1\nmistune 0.7.4\nmissingno 0.4.2\nmatplotlib 3.3.3\nMarkupSafe 1.1.1\nMarkdown 3.1.1\nlxml 4.6.1\nlocket 0.2.0\nllvmlite 0.18.0\nlightgbm 2.2.3\nlibrary-metadata-cooker 0.0.1\nliac-arff 2.5.0\nlazy-object-proxy 1.2.2\nkiwisolver 1.3.1\nkeras2onnx 1.5.2\nKeras-Preprocessing 1.1.0\nKeras-Applications 1.0.8\njupyter 1.0.0\njupyter-core 4.3.0\njupyter-console 5.1.0\njupyter-client 5.0.1\nJsonSir 0.0.2\njsonschema 3.1.1\njsonpickle 1.2\nJsonForm 0.0.2\njson-logging-py 0.2\njoblib 0.14.1\njmespath 0.9.4\nJinja2 2.10.3\njeepney 0.4.1\njedi 0.17.2\njdcal 1.3\nitsdangerous 0.24\nisort 4.2.5\nisodate 0.6.0\nipywidgets 6.0.0\nipython 7.16.1\nipython-genutils 0.2.0\nipykernel 4.6.1\ninterpret-core 0.2.1\ninterpret-community 0.14.4\nimportlib-metadata 0.23\nimagesize 0.7.1\nimageio 2.6.1\nidna 2.10\nhtml5lib 0.999\nHeapDict 1.0.1\nh5py 2.10.0\ngunicorn 19.9.0\ngrpcio 1.24.1\ngreenlet 0.4.12\ngoogle-pasta 0.1.7\ngevent 1.2.1\ngensim 3.8.1\ngast 0.3.2\nfusepy 3.0.1\nfsspec 0.5.2\nFlask 1.0.3\nFlask-Cors 3.0.2\nfire 0.2.1\nfastcache 1.0.2\net-xmlfile 1.0.1\nentrypoints 0.3\nempyrical 0.5.5\ndotnetcore2 2.1.14\ndocutils 0.15.2\ndocker 4.1.0\ndistro 1.4.0\ndistributed 1.16.3\ndill 0.3.1.1\ndecorator 4.4.2\ndatashape 0.5.4\ndataclasses 0.7\ndask 0.14.3\ncytoolz 0.8.2\nCython 0.29.13\ncycler 0.10.0\ncryptography 2.7\ncontextlib2 0.6.0.post1\nconfigparser 3.7.4\ncolorama 0.3.9\nclyent 1.2.2\ncloudpickle 1.2.2\nclick 6.7\nchart-studio 1.0.0\nchardet 3.0.4\ncffi 1.12.3\ncertifi 2020.11.8\nBottleneck 1.2.1\nbotocore 1.12.247\nboto3 1.9.247\nboto 2.49.0\nbokeh 1.3.4\nbleach 1.5.0\nblaze 0.10.1\nbitarray 0.8.1\nbeautifulsoup4 4.6.0\nbackports.weakref 1.0.post1\nbackports.tempfile 1.0\nbackports.shutil-get-terminal-size 1.0.0\nbackcall 0.2.0\nBabel 2.4.0\nazureml-train 1.10.0\nazureml-train-restclients-hyperdrive 1.10.0\nazureml-train-core 1.10.0\nazureml-train-automl 1.10.0\nazureml-train-automl-runtime 1.10.0\nazureml-train-automl-client 1.10.0\nazureml-telemetry 1.10.0\nazureml-sdk 1.10.0\nazureml-pipeline 1.10.0\nazureml-pipeline-steps 1.10.0\nazureml-pipeline-core 1.10.0\nazureml-opendatasets 1.10.0\nazureml-model-management-sdk 1.0.1b6.post1\nazureml-interpret 1.10.0\nazureml-explain-model 1.10.0\nazureml-defaults 1.10.0\nazureml-dataset-runtime 1.10.0\nazureml-dataprep 1.10.1\nazureml-dataprep-native 14.2.1\nazureml-core 1.10.0.post1\nazureml-automl-runtime 1.10.0.post1\nazureml-automl-core 1.10.0.post1\nazure-storage-common 2.1.0\nazure-storage-blob 2.1.0\nazure-mgmt-storage 4.2.0\nazure-mgmt-resource 5.1.0\nazure-mgmt-keyvault 2.0.0\nazure-mgmt-containerregistry 2.8.0\nazure-mgmt-authorization 0.60.0\nazure-identity 1.2.0\nazure-graphrbac 0.61.1\nazure-core 1.9.0\nazure-common 1.1.26\nazure-ai-textanalytics 5.0.0\nattrs 19.2.0\nastropy 1.3.2\nastroid 1.4.9\nastor 0.8.0\nasn1crypto 1.0.1\napplicationinsights 0.11.9\nappdirs 1.4.4\naltair 3.2.0\nalphalens 0.4.0\nalabaster 0.7.10\nadal 1.2.2\nabsl-py 0.8.1\nSphinx 1.5.6"
          },
          "execution_count": 1,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import pip #needed to use the pip functions\n",
        "for i in pip.get_installed_distributions(local_only=True):print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "marketriskspksm",
              "session_id": 73,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-11-18T22:01:20.2406317Z",
              "execution_start_time": "2020-11-18T22:01:20.2778685Z",
              "execution_finish_time": "2020-11-18T22:01:22.3182875Z"
            },
            "text/plain": "StatementMeta(marketriskspksm, 73, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 2,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import urllib.parse, base64\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import pytz\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "\n",
        "#sentiment_url = 'https://#COGNITIVE_SERVICE_NAME#.api.cognitive.microsoft.com/text/analytics/v3.0/sentiment' # service address \n",
        "sentiment_url = 'https://#COGNITIVE_SERVICE_NAME#.cognitiveservices.azure.com/'\n",
        "api_key = '#COGNITIVE_SERVICE_KEY#'          # Azure Cognitive API Key, replace with your own key\n",
        "\n",
        "\n",
        "credential = AzureKeyCredential(api_key)\n",
        "text_analytics_client = TextAnalyticsClient(endpoint=sentiment_url, credential=credential)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "marketriskspksm",
              "session_id": 73,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-11-18T22:01:27.0488756Z",
              "execution_start_time": "2020-11-18T22:01:27.0866058Z",
              "execution_finish_time": "2020-11-18T22:01:29.125209Z"
            },
            "text/plain": "StatementMeta(marketriskspksm, 73, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from azure.storage.blob import (\n",
        "    BlockBlobService\n",
        ")\n",
        "\n",
        "accountName = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "accountKey = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "containerName = \"risk\"\n",
        "\n",
        "blobService = BlockBlobService(account_name=accountName, account_key=accountKey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "marketriskspksm",
              "session_id": 73,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-11-18T22:01:48.6167655Z",
              "execution_start_time": "2020-11-18T22:01:48.6485547Z",
              "execution_finish_time": "2020-11-18T22:01:50.6786443Z"
            },
            "text/plain": "StatementMeta(marketriskspksm, 73, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "TwitterData/2020/11/17"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import datetime\n",
        "import pytz\n",
        "\n",
        "currDate = datetime.datetime.now(pytz.timezone('US/Central'))\n",
        "blobName = \"TwitterData/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day - 1)\n",
        "print(blobName)\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "marketriskspksm",
              "session_id": 73,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-11-18T22:01:57.5373686Z",
              "execution_start_time": "2020-11-18T22:01:57.5703554Z",
              "execution_finish_time": "2020-11-18T22:02:28.0513434Z"
            },
            "text/plain": "StatementMeta(marketriskspksm, 73, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "6e907496-9d5d-4f64-bff5-f182dde8830d",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, 6e907496-9d5d-4f64-bff5-f182dde8830d)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  'JavaPackage' object is not callable\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {},
          "execution_count": 5,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Read out CSV file with list of all tickers\r\n",
        "nqSdf = spark.read.load('abfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/Portfolio/PortfolioCik.csv', \r\n",
        "    format='csv', \r\n",
        "    sep=\",\",\r\n",
        "    header=True)\r\n",
        "\r\n",
        "portfolioData = nqSdf.toPandas()\r\n",
        "display(portfolioData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "tester",
              "session_id": 10,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-10-28T16:38:47.88847Z",
              "execution_start_time": "2020-10-28T16:38:47.9219154Z",
              "execution_finish_time": "2020-10-28T16:39:30.7596703Z"
            },
            "text/plain": "StatementMeta(tester, 10, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "abfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/AMZN.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/ARE.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/CNP.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/COST.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/CRM.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/CVS.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/DIS.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/DPZ.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/FISV.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/FMC.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/GIS.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/HRL.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/KMB.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/LMT.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/MDLZ.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/MKC.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/MSFT.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/NFLX.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/PG.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/RNG.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/TGT.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/WMT.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/ZM.csv\nabfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/TwitterData/2020/10/28/11/ZS.csv\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "b5ba4d63-48f5-4464-9798-b450c2c26451",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, b5ba4d63-48f5-4464-9798-b450c2c26451)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  'JavaPackage' object is not callable\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  Tried to write record batch with different schema\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {},
          "execution_count": 16,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import glob\n",
        "import re \n",
        "\n",
        "filename_prefix = \"/\" + blobName \n",
        "filename_objects = blobService.list_blobs('risk', prefix = filename_prefix)\n",
        "#print(filename_objects)\n",
        "\n",
        "for index, portfolio in portfolioData.iterrows():\n",
        "   print(portfolio.Ticker)\n",
        "   if portfolio.Ticker :\n",
        "      filtered = [fileinfo.name for fileinfo in filename_objects if portfolio.Ticker in fileinfo.name]\n",
        "      print(filtered)\n",
        "      consolidatedtweets = pd.DataFrame()\n",
        "\n",
        "      for tickerfilename in filtered : \n",
        "        absolouteFileName = \"abfss://risk@#DATA_LAKE_NAME#.dfs.core.windows.net/\" + tickerfilename\n",
        "        print(absolouteFileName)\n",
        "        nqSdf = spark.read.format('csv') \\\n",
        "                    .option('header',True) \\\n",
        "                    .option('multiLine', True) \\\n",
        "                    .load(absolouteFileName)\n",
        "        \n",
        "        consolidatedtweets = consolidatedtweets.append(nqSdf.toPandas())\n",
        "        scoredTweets = pd.DataFrame()\n",
        "        consolidatedtweets['Location'] = consolidatedtweets['Location\\r'].str.strip(r'\\\\r').astype(str)\n",
        "        consolidatedtweets = consolidatedtweets.drop(columns='Location\\r')\n",
        "        \n",
        "      print(\"Before drop\", consolidatedtweets.shape[0])\n",
        "      consolidatedtweets = consolidatedtweets.drop_duplicates(subset=['TweetText'])\n",
        "      print(\"After Drop\", consolidatedtweets.shape[0])\n",
        "\n",
        "      for index, tweet in consolidatedtweets.iterrows():\n",
        "            documents = []\n",
        "            mentions = re.findall(\"@([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
        "            topics = re.findall(\"#([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
        "            documents.append (\"'\" + tweet.TweetText + \"'\")\n",
        "            tweet['mentions'] = mentions\n",
        "            tweet['topics'] = topics\n",
        "            tweet['Ticker'] = portfolio.Ticker\n",
        "            response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
        "            result = [doc for doc in response if not doc.is_error]\n",
        "            tweet['sentiment'] = result[0].sentiment\n",
        "            tweet['positive'] = result[0].confidence_scores.positive\n",
        "            tweet['negative'] = result[0].confidence_scores.negative\n",
        "            tweet['neutral'] = result[0].confidence_scores.neutral\n",
        "\n",
        "            if tweet.Location : \n",
        "              tweet.Location = tweet.Location.rstrip('\\r')\n",
        "            else :\n",
        "              tweet.Location = \" \"\n",
        "              tweet.Location = tweet.Location.rstrip('\\r')\n",
        "            \n",
        "            scoredTweets = scoredTweets.append(tweet,ignore_index=True) \n",
        "\n",
        "      output = scoredTweets.to_csv(index=False, header=True, encoding = \"utf-8\")\n",
        "      blobService.create_blob_from_text(containerName,  \"TwitterData/scored/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day) + \"/\" + portfolio.Ticker + \".csv\", output)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "           \n",
        "\n",
        "\n",
        "  \n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}